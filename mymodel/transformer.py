import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import pandas as pd
import numpy as np
import rtdl
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, mean_squared_error
import time
import os
import random
import sys
import json

# --- Hydra å¯¼å…¥ ---
import hydra
from omegaconf import DictConfig, OmegaConf
from hydra.utils import get_original_cwd, to_absolute_path # 

# --- 0. é…ç½®ä¸è®¾ç½®éšæœºç§å­ ---
def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# --- è‡ªå®šä¹‰æ¨¡å‹ç±» ---
class MyFTTransformer(nn.Module):
    def __init__(self, ft_transformer_module):
        super().__init__()
        self.ft_transformer = ft_transformer_module

    def forward(self, x_num, x_cat):
        out = self.ft_transformer(x_num, x_cat)

        if isinstance(out, tuple):
            x_embed, x_out = out   
        else:
            x_out = out           

        # å›å½’ä»»åŠ¡ d_out=1 æ—¶ï¼Œåšä¸ª squeeze
        if x_out.shape[-1] == 1:
            return x_out.squeeze(-1)
        return x_out


# --- 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† (é‡æ„ä¸ºå‡½æ•°) ---
def load_and_preprocess_data(cfg: DictConfig, batch_size: int):
    """
    æ ¹æ® config å¯¹è±¡åŠ è½½å’Œé¢„å¤„ç†æ•°æ®ã€‚
    
    (é‡æ„ç‰ˆ) 
    æ­¤å‡½æ•°å‡è®¾æ‰€æœ‰æ•°æ®é›†éƒ½å·²é€šè¿‡æ ‡å‡†é¢„å¤„ç†è„šæœ¬å¤„ç†ã€‚
    å®ƒå°†åŠ è½½å·²å¤„ç†çš„ .csv (ç‰¹å¾) å’Œ .pt (æ ‡ç­¾) æ–‡ä»¶ï¼Œ
    å¹¶æ ¹æ® cfg.num_con å’Œ cfg.num_cat æ‹†åˆ†æ•°å€¼å’Œåˆ†ç±»ç‰¹å¾ã€‚
    """
    
    print(f"--- 1. æ­£åœ¨ä¸ºæ•°æ®é›†: '{cfg.target}' (é€šç”¨åŠ è½½å™¨) åŠ è½½æ•°æ® ---")

    # ==================================================================
    # == é€šç”¨æ•°æ®åŠ è½½é€»è¾‘
    # ==================================================================
    
    try:
        # --- 1. ä» config ä¸­è·å–å…³é”®å…ƒæ•°æ® ---
        num_con = cfg.num_con
        num_cat = cfg.num_cat
        total_features = num_con + num_cat
        print(f"    é…ç½®: {num_con} ä¸ªè¿ç»­ç‰¹å¾, {num_cat} ä¸ªåˆ†ç±»ç‰¹å¾ã€‚")

        # --- 2. åŠ è½½ç‰¹å¾ (CSV, æ— è¡¨å¤´) ---
        # é¢„å¤„ç†è„šæœ¬å·²ç¡®ä¿åˆ—é¡ºåºä¸º [è¿ç»­ç‰¹å¾, åˆ†ç±»ç‰¹å¾]
        train_df = pd.read_csv(cfg.data_train_eval_tabular, header=None)
        val_df = pd.read_csv(cfg.data_val_eval_tabular, header=None)
        test_df = pd.read_csv(cfg.data_test_eval_tabular, header=None)

        # éªŒè¯ç‰¹å¾æ•°é‡æ˜¯å¦åŒ¹é…
        if train_df.shape[1] != total_features:
            print(f"ğŸ”´ é”™è¯¯ï¼šåŠ è½½çš„ train_df æœ‰ {train_df.shape[1]} åˆ—, ä½† config é¢„æœŸ {total_features} (num_con+num_cat) åˆ—ã€‚")
            print(f"    è¯·æ£€æŸ¥: {cfg.data_train_eval_tabular}")
            sys.exit(1)

        # --- 3. åŠ è½½æ ‡ç­¾ (PyTorch Tensors) ---
        y_train = torch.load(cfg.labels_train_eval_tabular)
        y_val = torch.load(cfg.labels_val_eval_tabular)
        y_test = torch.load(cfg.labels_test_eval_tabular)

        # --- 4. åŠ è½½å­—æ®µé•¿åº¦ (åŸºæ•°) ---
        # é¢„å¤„ç†è„šæœ¬ä¿å­˜çš„æ ¼å¼: [1, 1, ..., 1 (num_con ä¸ª), card1, card2, ... (num_cat ä¸ª)]
        all_field_lengths = torch.load(cfg.field_lengths_tabular)
        
        # ä»…æå–åˆ†ç±»ç‰¹å¾çš„åŸºæ•°
        cat_cardinalities = all_field_lengths[num_con:]
        
        # éªŒè¯åŸºæ•°åˆ—è¡¨é•¿åº¦æ˜¯å¦åŒ¹é…
        if len(cat_cardinalities) != num_cat:
            print(f"ğŸ”´ é”™è¯¯ï¼š'tabular_lengths.pt' ä¸­æå–äº† {len(cat_cardinalities)} ä¸ªåŸºæ•°, ä½† config é¢„æœŸ {num_cat} ä¸ªåˆ†ç±»ç‰¹å¾ã€‚")
            print(f"    åŠ è½½çš„åˆ—è¡¨: {all_field_lengths}")
            sys.exit(1)
        
        print(f"    æˆåŠŸåŠ è½½ {len(cat_cardinalities)} ä¸ªåˆ†ç±»ç‰¹å¾çš„åŸºæ•°: {cat_cardinalities}")

        
    except FileNotFoundError as e:
        print(f"ğŸ”´ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {e.filename}ã€‚")
        print("    è¯·ç¡®ä¿ config ä¸­çš„è·¯å¾„æ­£ç¡®ï¼Œå¹¶ä¸”é¢„å¤„ç†è„šæœ¬å·²æˆåŠŸè¿è¡Œã€‚")
        sys.exit(1)
    except Exception as e:
        print(f"ğŸ”´ åŠ è½½æ•°æ®æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        sys.exit(1)

    # --- 5. æ‹†åˆ†ç‰¹å¾å¹¶è½¬æ¢ä¸º Tensors ---
    
    def split_and_convert_to_tensors(df, y_tensor):
        """
        æ ¹æ® num_con æ‹†åˆ† dfï¼Œå¹¶è½¬æ¢ä¸º (num_tensor, cat_tensor, y_tensor)
        
        æ³¨æ„ï¼šæˆ‘ä»¬åŠ è½½çš„æ˜¯ _TIP.csv æ–‡ä»¶ï¼Œå®ƒå¯èƒ½åŒ…å« NaNs (ç”¨äºè¿ç»­ç‰¹å¾) 
        æˆ– -1 (ç”¨äºåˆ†ç±»ç‰¹å¾)ã€‚
        .astype(np.float32) ä¼šä¿ç•™ NaNsã€‚
        .astype(np.int64) ä¼šä¿ç•™ -1ã€‚
        è¿™æ˜¯é¢„æœŸè¡Œä¸ºã€‚
        """
        
        # æ‹†åˆ†æ•°å€¼å’Œåˆ†ç±»
        X_num_df = df.iloc[:, :num_con]
        X_cat_df = df.iloc[:, num_con:]
        
        # è½¬æ¢ä¸º Tensors
        # (æˆ‘ä»¬å‡è®¾ _TIP æ–‡ä»¶ä¸­çš„ NaN å’Œ -1 æ˜¯æ¨¡å‹è¾“å…¥çš„ä¸€éƒ¨åˆ†)
        X_num_tensor = torch.tensor(X_num_df.values.astype(np.float32))
        X_cat_tensor = torch.tensor(X_cat_df.values.astype(np.int64))
        
        # ç¡®ä¿æ ‡ç­¾æ˜¯ long ç±»å‹ (ç”¨äºåˆ†ç±») æˆ– float (ç”¨äºå›å½’)
        if cfg.task == 'classification':
            y_tensor = y_tensor.long()
        else:
            y_tensor = y_tensor.float()

        return X_num_tensor, X_cat_tensor, y_tensor

    print("    æ­£åœ¨å°†æ•°æ®å¸§ (DataFrame) è½¬æ¢ä¸ºå¼ é‡ (Tensors)...")
    X_train_num, X_train_cat, y_train = split_and_convert_to_tensors(train_df, y_train)
    X_val_num, X_val_cat, y_val = split_and_convert_to_tensors(val_df, y_val)
    X_test_num, X_test_cat, y_test = split_and_convert_to_tensors(test_df, y_test)
    
    print("    æ•°æ®åŠ è½½å’Œå¤„ç†å®Œæˆã€‚")

    # ==================================================================
    # == åˆ›å»º DataLoaders (é€šç”¨)
    # ==================================================================
    print("--- 2. æ­£åœ¨åˆ›å»º DataLoaders ---")
    
    try:
        train_dataset = TensorDataset(X_train_num, X_train_cat, y_train)
        val_dataset = TensorDataset(X_val_num, X_val_cat, y_val)
        test_dataset = TensorDataset(X_test_num, X_test_cat, y_test)
    except Exception as e:
        print(f"ğŸ”´ åˆ›å»º TensorDataset æ—¶å‡ºé”™: {e}")
        print("    è¯·æ£€æŸ¥æ‰€æœ‰ train/val/test æ–‡ä»¶çš„æ ·æœ¬é‡æ˜¯å¦ä¸€è‡´ã€‚")
        sys.exit(1)
        
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    print("DataLoaders åˆ›å»ºå®Œæˆã€‚")

    # --- 3. å‡†å¤‡æ¨¡å‹è¾“å…¥ ---
    
    # æ¨¡å‹è¾“å‡ºç»´åº¦
    d_out = cfg.num_classes
    
    # è¿”å›æ‰€æœ‰å¿…è¦ç»„ä»¶
    model_inputs = {
        "n_num_features": num_con,          # æ•°å€¼ç‰¹å¾çš„æ•°é‡
        "cat_cardinalities": cat_cardinalities, # åˆ†ç±»ç‰¹å¾åŸºæ•°åˆ—è¡¨
        "d_out": d_out,
        "task": cfg.task
    }
    loaders = {
        "train": train_loader,
        "val": val_loader,
        "test": test_loader
    }
    
    return loaders, model_inputs

# --- 3. å®šä¹‰æ¨¡å‹åˆ›å»ºå‡½æ•° (å·²ä¿®æ­£ï¼šè¡¥å…¨ rtdl.Transformer æ‰€éœ€çš„å‚æ•°) ---
def create_model(params, n_num_features, cat_cardinalities, d_out, device):
    base_ft_transformer = rtdl.FTTransformer(
        feature_tokenizer=rtdl.modules.FeatureTokenizer(
            n_num_features=n_num_features, 
            cat_cardinalities=cat_cardinalities, 
            d_token=params['d_token']
        ),
        transformer=rtdl.modules.Transformer(
            d_token=params['d_token'],
            n_blocks=params['n_blocks'],
            attention_dropout=params['attention_dropout'],
            ffn_d_hidden=params['ffn_d_hidden'],
            ffn_dropout=params['ffn_dropout'],
            residual_dropout=params['residual_dropout'],

            attention_n_heads=8,
            attention_initialization='kaiming',
            attention_normalization='LayerNorm',
            ffn_activation='ReLU',
            ffn_normalization='LayerNorm',
            prenormalization=True,
            first_prenormalization=False,
            last_layer_query_idx=[-1],
            n_tokens=None,
            kv_compression_ratio=None,
            kv_compression_sharing=None,
            head_activation=nn.Identity,
            head_normalization=nn.Identity,

            # ğŸ”´ å…³é”®ï¼šè¿™é‡Œçš„ d_out ä¸€å®šè¦æ˜¯ â€œæœ€ç»ˆè¾“å‡ºç»´åº¦â€ï¼Œä¹Ÿå°±æ˜¯ num_classes
            d_out=d_out,
        ),
    )

    model = MyFTTransformer(
        ft_transformer_module=base_ft_transformer,
    ).to(device)

    return model


# --- 4. è¾…åŠ©å‡½æ•°ï¼šè·å–æŸå¤±å‡½æ•°å’Œè¯„ä¼°æŒ‡æ ‡ ---
def create_loss_fn(task, device):
    if task == 'classification':
        return nn.CrossEntropyLoss().to(device)
    elif task == 'regression':
        return nn.MSELoss().to(device)
    else:
        raise ValueError(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task}")

def get_scoring_info(task):
    """è·å–é˜¶æ®µä¸€æœç´¢æ‰€éœ€çš„è¯„ä¼°æŒ‡æ ‡å’Œä¼˜åŒ–æ–¹å‘"""
    if task == 'classification':
        return 'accuracy', 'max' # æŒ‡æ ‡åç§°, ä¼˜åŒ–æ–¹å‘
    elif task == 'regression':
        return 'rmse', 'min'
    else:
        raise ValueError(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task}")

# --- 5. å®šä¹‰è®­ç»ƒä¸è¯„ä¼°å‡½æ•° ---

# é˜¶æ®µä¸€å‡½æ•°ï¼šå¿«é€Ÿæœç´¢
def search_for_best_params(param_combinations, cfg, seed, loaders, model_inputs, device):
    print("\n" + "-"*10 + f" [ç§å­ {seed}] é˜¶æ®µä¸€ï¼šå¿«é€Ÿè¶…å‚æ•°æœç´¢ (15 epochs) " + "-"*10)
    
    train_loader, val_loader = loaders['train'], loaders['val']
    n_num, cats, d_out, task = model_inputs.values()
    
    scoring_metric, mode = get_scoring_info(task)
    best_score = -float('inf') if mode == 'max' else float('inf')
    best_params = None
    
    for i, params in enumerate(param_combinations):
        print(f"\n--- [è¯•éªŒ {i+1}/{len(param_combinations)}] ---")
        print(f"æµ‹è¯•å‚æ•°: {params}")
        
        set_seed(seed)
        model = create_model(params, n_num, cats, d_out, device)
        optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])
        loss_fn = create_loss_fn(task, device)
        
        # è®­ç»ƒå›ºå®šçš„15ä¸ªepoch
        for epoch in range(15):
            model.train()
            for x_num_batch, x_cat_batch, y_batch in train_loader:
                x_num_batch, x_cat_batch, y_batch = x_num_batch.to(device), x_cat_batch.to(device), y_batch.to(device)
                optimizer.zero_grad()
                y_pred = model(x_num_batch, x_cat_batch)
                # print("y_pred shape:", y_pred.shape, "dtype:", y_pred.dtype)
                # print("y_batch shape:", y_batch.shape, "dtype:", y_batch.dtype)

                loss = loss_fn(y_pred, y_batch.long())
                loss.backward()
                optimizer.step()
                

        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        model.eval()
        val_preds_proba = []
        val_labels = []
        with torch.no_grad():
            for x_num_batch, x_cat_batch, y_batch in val_loader:
                x_num_batch, x_cat_batch = x_num_batch.to(device), x_cat_batch.to(device)
                y_pred = model(x_num_batch, x_cat_batch)
                
                # ç»Ÿä¸€å¤„ç†: proba ç”¨äºåˆ†ç±», value ç”¨äºå›å½’
                if task == 'classification':
                    val_preds_proba.append(y_pred.softmax(dim=1).cpu().numpy())
                else:
                    val_preds_proba.append(y_pred.cpu().numpy()) # (N,) or (N, 1)
                val_labels.append(y_batch.cpu().numpy())
        
        val_preds_proba = np.concatenate(val_preds_proba)
        val_labels = np.concatenate(val_labels)
        
        # åŠ¨æ€è®¡ç®—å¾—åˆ†
        current_score = 0.0
        if task == 'classification':
            val_preds_class = np.argmax(val_preds_proba, axis=1)
            current_score = accuracy_score(val_labels, val_preds_class)
        elif task == 'regression':
            current_score = np.sqrt(mean_squared_error(val_labels, val_preds_proba.squeeze()))
        
        print(f"è¯•éªŒ {i+1} éªŒè¯é›† {scoring_metric}: {current_score:.4f}")
        
        if (mode == 'max' and current_score > best_score) or \
           (mode == 'min' and current_score < best_score):
            best_score = current_score
            best_params = params
            print(f"  (å‘ç°æ–°çš„æœ€ä½³å‚æ•°!)")
            
    print("\n" + "-"*10 + " é˜¶æ®µä¸€æœç´¢å®Œæˆ " + "-"*10)
    print(f"æœ€ä½³éªŒè¯é›† {scoring_metric}: {best_score:.4f}")
    print(f"é€‰å®šçš„æœ€ä½³å‚æ•°: {best_params}")
    return best_params

# é˜¶æ®µäºŒå‡½æ•°ï¼šä½¿ç”¨æ—©åœå……åˆ†è®­ç»ƒ
def train_final_model(best_params, cfg, seed, loaders, model_inputs, device, 
                      patience: int, max_epochs: int):
    print("\n" + "-"*10 + f" [ç§å­ {seed}] é˜¶æ®µäºŒï¼šä½¿ç”¨æ—©åœæœºåˆ¶å……åˆ†è®­ç»ƒæœ€ä½³æ¨¡å‹ " + "-"*10)
    
    train_loader, val_loader = loaders['train'], loaders['val']
    n_num, cats, d_out, task = model_inputs.values()
    
    set_seed(seed)
    model = create_model(best_params, n_num, cats, d_out, device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])
    loss_fn = create_loss_fn(task, device)

    best_val_loss = float('inf')
    patience_counter = 0
    best_model_path = f'best_model_seed_{seed}.pt' 
    # [!] ä½¿ç”¨æ¥è‡ª json çš„å‚æ•°
    # max_epochs = cfg.hyperparams.max_epochs (ç§»é™¤)
    # patience = cfg.hyperparams.patience (ç§»é™¤)

    for epoch in range(max_epochs):
        model.train()
        for x_num_batch, x_cat_batch, y_batch in train_loader:
            x_num_batch, x_cat_batch, y_batch = x_num_batch.to(device), x_cat_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            y_pred = model(x_num_batch, x_cat_batch)
            loss = loss_fn(y_pred, y_batch.long())
            loss.backward()
            optimizer.step()
        
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for x_num_batch, x_cat_batch, y_batch in val_loader:
                x_num_batch, x_cat_batch, y_batch = x_num_batch.to(device), x_cat_batch.to(device), y_batch.to(device)
                y_pred = model(x_num_batch, x_cat_batch)
                val_loss += loss_fn(y_pred, y_batch.long()).item()
        val_loss /= len(val_loader)
        print(f"Epoch {epoch + 1}/{max_epochs}, Val Loss: {val_loss:.4f}")

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            torch.save(model.state_dict(), best_model_path)
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"  (éªŒè¯é›†æŸå¤±è¿ç»­ {patience} ä¸ªepochæœªæ”¹å–„ï¼Œè§¦å‘æ—©åœ!)")
                break
    
    print(f"åŠ è½½åœ¨éªŒè¯é›†ä¸Šæ€§èƒ½æœ€ä½³çš„æ¨¡å‹ (æ¥è‡ª {best_model_path})...")
    model.load_state_dict(torch.load(best_model_path))
    if os.path.exists(best_model_path):
        os.remove(best_model_path)
        
    return model

# é˜¶æ®µä¸‰å‡½æ•°ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
def evaluate_final_model(final_model, test_loader, task, device):
    final_model.eval()
    all_preds_proba, all_labels = [], []
    with torch.no_grad():
        for x_num_batch, x_cat_batch, y_batch in test_loader:
            x_num_batch, x_cat_batch = x_num_batch.to(device), x_cat_batch.to(device)
            
            y_pred = final_model(x_num_batch, x_cat_batch)
            
            if task == 'classification':
                all_preds_proba.append(y_pred.softmax(dim=1).cpu().numpy())
            else:
                all_preds_proba.append(y_pred.cpu().numpy())
            all_labels.append(y_batch.cpu().numpy())
            
    all_preds_proba = np.concatenate(all_preds_proba)
    all_labels = np.concatenate(all_labels)

    metrics_dict = {}
    result_line = ""

    if task == 'classification':
        all_preds_class = np.argmax(all_preds_proba, axis=1)
        acc = accuracy_score(all_labels, all_preds_class)
        auc = roc_auc_score(all_labels, all_preds_proba, multi_class='ovr', average='macro')
        macro_f1 = f1_score(all_labels, all_preds_class, average='macro')
        
        metrics_dict = {'acc': acc, 'auc': auc, 'macro-F1': macro_f1}
        result_line = f"acc:{acc:.4f},auc:{auc:.4f},macro-F1:{macro_f1:.4f}"
        
    elif task == 'regression':
        rmse = np.sqrt(mean_squared_error(all_labels, all_preds_proba.squeeze()))
        metrics_dict = {'rmse': rmse}
        result_line = f"rmse:{rmse:.4f}"

    print(f"æœ€ç»ˆæµ‹è¯•é›†æ€§èƒ½: {result_line}")
    return metrics_dict, result_line

# --- 6. ä¸»æ‰§è¡Œæµç¨‹ (Hydra Main) ---
@hydra.main(version_base=None, config_path="../configs", config_name="config")
def main(cfg: DictConfig):

    print("--- 1.A. æ­£åœ¨è§£ææ•°æ®è·¯å¾„ ---")
    
    # 1. ä»å‘½ä»¤è¡Œè·å– 'data_root'
    #    (å¦‚æœæœªæä¾›ï¼Œdata_root å°†ä¸º None)
    data_root = cfg.get('data_base')

    if data_root is not None:
        print(f"    æ£€æµ‹åˆ° 'data_root'ï¼Œå°†ä¸ºæ‰€æœ‰æ•°æ®æ–‡ä»¶æ·»åŠ å‰ç¼€: {data_root}")
        
        # 2. å®šä¹‰åœ¨ .yaml ä¸­æ‰€æœ‰â€œéœ€è¦â€æ·»åŠ å‰ç¼€çš„è·¯å¾„é”®
        path_keys = [
            'labels_train', 'labels_val',
            'data_train_imaging', 'data_val_imaging',
            'data_train_tabular', 'data_val_tabular',
            'field_lengths_tabular',
            'data_train_eval_tabular', 'labels_train_eval_tabular',
            'data_val_eval_tabular', 'labels_val_eval_tabular',
            'data_test_eval_tabular', 'labels_test_eval_tabular',
            'data_train_eval_imaging', 'labels_train_eval_imaging',
            'data_val_eval_imaging', 'labels_val_eval_imaging',
            'data_test_eval_imaging', 'labels_test_eval_imaging'
        ]
        
        # 3. éå†è¿™äº›é”®ï¼Œå¦‚æœå®ƒä»¬å­˜åœ¨äº cfg ä¸­ï¼Œåˆ™æ›´æ–°è·¯å¾„
        for key in path_keys:
            if key in cfg and cfg[key] is not None:
                original_path = cfg[key]
                absolute_path = os.path.join(data_root, original_path)
                cfg[key] = absolute_path # [!] ç›´æ¥ä¿®æ”¹ config å¯¹è±¡
                # print(f"        {key}: {original_path} -> {absolute_path}") # (å¯é€‰ï¼šç”¨äºè°ƒè¯•)
            
    else:
        print("    æœªæä¾› 'data_root'ã€‚å°†å‡å®š config ä¸­çš„è·¯å¾„å·²ç»æ˜¯æ­£ç¡®çš„ (ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹äºCWD)ã€‚")
    
    print("--- æœ€ç»ˆé…ç½®: ---")
    print(OmegaConf.to_yaml(cfg))
    print("--------------------")
    print(f"Hydra å·¥ä½œç›®å½•: {os.getcwd()}")
    print("--------------------")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'æ­£åœ¨ä½¿ç”¨è®¾å¤‡: {device}')
    
    original_cwd = get_original_cwd() 
    model_config_filename = 'ftt_model_config.json' # [!] æ–‡ä»¶åä¿®æ”¹
    model_config_path = os.path.join(original_cwd, model_config_filename)
    
    print(f"--- æ­£åœ¨ä» {model_config_path} åŠ è½½æ¨¡å‹é…ç½® ---")
    try:
        with open(model_config_path, 'r') as f:
            model_config = json.load(f)
        search_space = model_config['search_space']
        hyperparams = model_config['hyperparams']
        print("æ¨¡å‹é…ç½® (hyperparams å’Œ search_space) åŠ è½½æˆåŠŸã€‚")
    except FileNotFoundError:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹é…ç½®æ–‡ä»¶: {model_config_path}")
        sys.exit(1)
    except KeyError as e:
        print(f"é”™è¯¯: æ¨¡å‹é…ç½®æ–‡ä»¶ '{model_config_path}' ç¼ºå°‘é”®: {e}")
        sys.exit(1)
    
    
    # 2. å°† hyperparams æå–åˆ°å˜é‡
    seeds = list(hyperparams['seeds'])
    batch_size = hyperparams['batch_size']
    D_TOKEN = hyperparams['d_token']
    LEARNING_RATE = hyperparams['learning_rate']
    WEIGHT_DECAY = hyperparams['weight_decay']
    N_TRIALS = hyperparams['n_trials']
    patience = hyperparams['patience']
    max_epochs = hyperparams['max_epochs']

    final_results_summary = []

    # 3. åŠ è½½æ•°æ®, ä¼ å…¥ batch_size
    loaders, model_inputs = load_and_preprocess_data(cfg, batch_size)

    for seed in seeds:
        print("\n" + "="*30 + f" å¼€å§‹æ‰§è¡Œï¼Œéšæœºç§å­: {seed} " + "="*30)
        set_seed(seed)

        # 4. ç”Ÿæˆéšæœºè¶…å‚æ•°ç»„åˆ (ä½¿ç”¨æ¥è‡ª JSON çš„ search_space)
        param_combinations = []
        for _ in range(N_TRIALS):
            params = {
                'n_blocks': random.choice(search_space['n_blocks']),
                'ffn_d_hidden': random.choice(search_space['ffn_d_hidden']),
                'residual_dropout': random.uniform(*search_space['residual_dropout']),
                'attention_dropout': random.uniform(*search_space['attention_dropout']),
                'ffn_dropout': random.uniform(*search_space['ffn_dropout']),
                'd_token': D_TOKEN, 
                'learning_rate': LEARNING_RATE, 
                'weight_decay': WEIGHT_DECAY,
            }
            param_combinations.append(params)
        
        # 5. é˜¶æ®µä¸€ï¼šæœç´¢
        best_params = search_for_best_params(
            param_combinations, cfg, seed, loaders, model_inputs, device
        )
        
        # [!] 6. é˜¶æ®µäºŒï¼šè®­ç»ƒ, ä¼ å…¥ patience å’Œ max_epochs
        final_model = train_final_model(
            best_params, cfg, seed, loaders, model_inputs, device,
            patience=patience, max_epochs=max_epochs
        )
        
        # 7. é˜¶æ®µä¸‰ï¼šè¯„ä¼°
        print("\n" + "-"*10 + f" [ç§å­ {seed}] é˜¶æ®µä¸‰ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼° " + "-"*10)
        metrics_dict, result_line = evaluate_final_model(
            final_model, loaders['test'], model_inputs['task'], device
        )
        
        result_dict = {
            'seed': seed, 
            'best_params': best_params, 
            'result_line': result_line,
            **metrics_dict
        }
        final_results_summary.append(result_dict)

    # --- 7. æœ€ç»ˆæ€»ç»“ ---
    print("\n\n" + "="*30 + " æ‰€æœ‰å®éªŒæœ€ç»ˆæ€»ç»“ " + "="*30)
    
    output_file_path = '/home/debian/TIP/mymodel/result/fttrans.txt'
    print(f"å‡†å¤‡å°†ç»“æœå†™å…¥åˆ°: {output_file_path}")

    output_dir = os.path.dirname(output_file_path)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    with open(output_file_path, 'a') as f:
        f.write(f"--- å®éªŒé…ç½® (æ¥è‡ª config.yaml): {cfg.target} ---\n")
        f.write(OmegaConf.to_yaml(cfg)) # å†™å…¥æ•°æ®é…ç½®
        f.write("\n--- æ¨¡å‹é…ç½® (æ¥è‡ª ftt_model_config.json) ---\n")
        f.write(json.dumps(model_config, indent=2)) # [!] å†™å…¥æ¨¡å‹é…ç½®
        f.write("\n\n" + "="*30 + " æ‰€æœ‰å®éªŒæœ€ç»ˆæ€»ç»“ " + "="*30 + "\n")
        
        for final_result in final_results_summary:
            result_line = f"ç§å­: {final_result['seed']} | {final_result['result_line']}"
            print(result_line)
            f.write(result_line + "\n")

        params_header = f"\næœ€ä½³å‚æ•°çš„ä¾‹å­ (æ¥è‡ªæœ€åä¸€ä¸ªç§å­ {final_results_summary[-1]['seed']}):"
        params_details = str(final_results_summary[-1]['best_params'])
        
        print(params_header)
        print(params_details)
        f.write(params_header + "\n")
        f.write(params_details + "\n")
        f.write("="*80 + "\n")

    print(f"\nç»“æœå·²æˆåŠŸå†™å…¥åˆ°æ–‡ä»¶: {output_file_path}")


if __name__ == "__main__":
    main()