import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import pandas as pd
import numpy as np
import rtdl
from sklearn.preprocessing import StandardScaler, LabelEncoder
# [ä¿®æ”¹ 1] å¼•å…¥ mean_absolute_error å’Œ r2_score
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, mean_squared_error, mean_absolute_error, r2_score
import time
import os
import random
import sys
import json

# --- Hydra å¯¼å…¥ ---
import hydra
from omegaconf import DictConfig, OmegaConf
from hydra.utils import get_original_cwd, to_absolute_path

# --- 0. é…ç½®ä¸è®¾ç½®éšæœºç§å­ ---
def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# --- è‡ªå®šä¹‰æ¨¡å‹ç±» ---
class MyFTTransformer(nn.Module):
    def __init__(self, ft_transformer_module):
        super().__init__()
        self.ft_transformer = ft_transformer_module

    def forward(self, x_num, x_cat):
        if x_num is not None and x_num.shape[1] == 0:
            x_num = None
            
        out = self.ft_transformer(x_num, x_cat)

        if isinstance(out, tuple):
            x_embed, x_out = out   
        else:
            x_out = out           

        # å›å½’ä»»åŠ¡ d_out=1 æ—¶ï¼Œåšä¸ª squeeze
        if x_out.shape[-1] == 1:
            return x_out.squeeze(-1)
        return x_out


# --- 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ---
def load_and_preprocess_data(cfg: DictConfig, batch_size: int):
    print(f"--- 1. æ­£åœ¨ä¸ºæ•°æ®é›†: '{cfg.target}' (é€šç”¨åŠ è½½å™¨) åŠ è½½æ•°æ® ---")

    try:
        # --- 1. åŠ è½½å­—æ®µé•¿åº¦ (åŸºæ•°) ---
        all_field_lengths = torch.load(cfg.field_lengths_tabular)
        
        if isinstance(all_field_lengths, torch.Tensor):
            all_field_lengths = all_field_lengths.cpu().tolist()
            
        con_indices = [i for i, length in enumerate(all_field_lengths) if length == 1]
        cat_indices = [i for i, length in enumerate(all_field_lengths) if length > 1]
        
        num_con = len(con_indices)
        num_cat = len(cat_indices)
        
        cat_cardinalities = [all_field_lengths[i] for i in cat_indices]

        print(f"    è‡ªåŠ¨æ£€æµ‹ç»“æœ: {num_con} ä¸ªè¿ç»­ç‰¹å¾, {num_cat} ä¸ªåˆ†ç±»ç‰¹å¾ã€‚")

        # --- 2. åŠ è½½ç‰¹å¾ (CSV) ---
        train_df = pd.read_csv(cfg.data_train_eval_tabular, header=None)
        val_df = pd.read_csv(cfg.data_val_eval_tabular, header=None)
        test_df = pd.read_csv(cfg.data_test_eval_tabular, header=None)

        if train_df.shape[1] != len(all_field_lengths):
            print(f"ğŸ”´ é”™è¯¯ï¼šCSV åˆ—æ•° ({train_df.shape[1]}) ä¸ field_lengths é•¿åº¦ ({len(all_field_lengths)}) ä¸ä¸€è‡´ï¼")
            sys.exit(1)

        # --- 3. åŠ è½½æ ‡ç­¾ ---
        y_train = torch.load(cfg.labels_train_eval_tabular)
        y_val = torch.load(cfg.labels_val_eval_tabular)
        y_test = torch.load(cfg.labels_test_eval_tabular)

    except Exception as e:
        print(f"ğŸ”´ åŠ è½½æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)

    # --- 4. æ‹†åˆ†ç‰¹å¾å¹¶è½¬æ¢ä¸º Tensors ---
    def split_and_convert_to_tensors(df, y_tensor):
        X_num_df = df.iloc[:, con_indices]
        X_cat_df = df.iloc[:, cat_indices]
        
        if len(con_indices) > 0:
            X_num_tensor = torch.tensor(X_num_df.values.astype(np.float32))
        else:
            X_num_tensor = None 

        X_cat_tensor = torch.tensor(X_cat_df.values.astype(np.int64))
        
        # æ ‡ç­¾å¤„ç†
        if cfg.task == 'classification':
            y_tensor = y_tensor.long()
        else:
            y_tensor = y_tensor.float()

        return X_num_tensor, X_cat_tensor, y_tensor

    print("    æ­£åœ¨æ ¹æ® field_lengths æ‹†åˆ†å¹¶è½¬æ¢æ•°æ®...")
    
    X_train_num, X_train_cat, y_train = split_and_convert_to_tensors(train_df, y_train)
    X_val_num, X_val_cat, y_val = split_and_convert_to_tensors(val_df, y_val)
    X_test_num, X_test_cat, y_test = split_and_convert_to_tensors(test_df, y_test)
    
    # DataLoader å¤„ç† None çš„æƒ…å†µ
    if X_train_num is None: X_train_num = torch.empty((len(y_train), 0))
    if X_val_num is None: X_val_num = torch.empty((len(y_val), 0))
    if X_test_num is None: X_test_num = torch.empty((len(y_test), 0))

    try:
        train_dataset = TensorDataset(X_train_num, X_train_cat, y_train)
        val_dataset = TensorDataset(X_val_num, X_val_cat, y_val)
        test_dataset = TensorDataset(X_test_num, X_test_cat, y_test)
    except Exception as e:
        print(f"ğŸ”´ åˆ›å»º Dataset å‡ºé”™: {e}")
        sys.exit(1)
        
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    d_out = cfg.num_classes
    
    model_inputs = {
        "n_num_features": num_con,
        "cat_cardinalities": cat_cardinalities,
        "d_out": d_out,
        "task": cfg.task
    }
    loaders = {
        "train": train_loader,
        "val": val_loader,
        "test": test_loader
    }
    
    return loaders, model_inputs

# --- 3. å®šä¹‰æ¨¡å‹åˆ›å»ºå‡½æ•° ---
def create_model(params, n_num_features, cat_cardinalities, d_out, device):
    base_ft_transformer = rtdl.FTTransformer(
        feature_tokenizer=rtdl.modules.FeatureTokenizer(
            n_num_features=n_num_features, 
            cat_cardinalities=cat_cardinalities, 
            d_token=params['d_token']
        ),
        transformer=rtdl.modules.Transformer(
            d_token=params['d_token'],
            n_blocks=params['n_blocks'],
            attention_dropout=params['attention_dropout'],
            ffn_d_hidden=params['ffn_d_hidden'],
            ffn_dropout=params['ffn_dropout'],
            residual_dropout=params['residual_dropout'],
            attention_n_heads=8,
            attention_initialization='kaiming',
            attention_normalization='LayerNorm',
            ffn_activation='ReLU',
            ffn_normalization='LayerNorm',
            prenormalization=True,
            first_prenormalization=False,
            last_layer_query_idx=[-1],
            n_tokens=None,
            kv_compression_ratio=None,
            kv_compression_sharing=None,
            head_activation=nn.Identity,
            head_normalization=nn.Identity,
            d_out=d_out,
        ),
    )

    model = MyFTTransformer(
        ft_transformer_module=base_ft_transformer,
    ).to(device)

    return model


# --- 4. è¾…åŠ©å‡½æ•° ---
def create_loss_fn(task, device):
    if task == 'classification':
        return nn.CrossEntropyLoss().to(device)
    elif task == 'regression':
        return nn.MSELoss().to(device)
    else:
        raise ValueError(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task}")

def get_scoring_info(task):
    if task == 'classification':
        return 'accuracy', 'max'
    elif task == 'regression':
        return 'rmse', 'min'
    else:
        raise ValueError(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task}")

# --- 5. å®šä¹‰è®­ç»ƒä¸è¯„ä¼°å‡½æ•° ---

def search_for_best_params(param_combinations, cfg, seed, loaders, model_inputs, device):
    print("\n" + "-"*10 + f" [ç§å­ {seed}] é˜¶æ®µä¸€ï¼šå¿«é€Ÿè¶…å‚æ•°æœç´¢ (15 epochs) " + "-"*10)
    
    train_loader, val_loader = loaders['train'], loaders['val']
    n_num, cats, d_out, task = model_inputs.values()
    
    scoring_metric, mode = get_scoring_info(task)
    best_score = -float('inf') if mode == 'max' else float('inf')
    best_params = None
    
    for i, params in enumerate(param_combinations):
        print(f"\n--- [è¯•éªŒ {i+1}/{len(param_combinations)}] ---")
        print(f"æµ‹è¯•å‚æ•°: {params}")
        
        set_seed(seed)
        model = create_model(params, n_num, cats, d_out, device)
        optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])
        loss_fn = create_loss_fn(task, device)
        
        for epoch in range(15):
            model.train()
            for x_num_batch, x_cat_batch, y_batch in train_loader:
                x_num_batch, x_cat_batch, y_batch = x_num_batch.to(device), x_cat_batch.to(device), y_batch.to(device)
                optimizer.zero_grad()
                y_pred = model(x_num_batch, x_cat_batch)
                
                # [ä¿®æ”¹ 2] ä¿®å¤ Loss ç±»å‹ï¼šå›å½’ç”¨ floatï¼Œåˆ†ç±»ç”¨ long
                target = y_batch.long() if task == 'classification' else y_batch.float()
                loss = loss_fn(y_pred, target)
                
                loss.backward()
                optimizer.step()

        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        model.eval()
        val_preds_proba = []
        val_labels = []
        with torch.no_grad():
            for x_num_batch, x_cat_batch, y_batch in val_loader:
                x_num_batch, x_cat_batch = x_num_batch.to(device), x_cat_batch.to(device)
                y_pred = model(x_num_batch, x_cat_batch)
                
                if task == 'classification':
                    val_preds_proba.append(y_pred.softmax(dim=1).cpu().numpy())
                else:
                    val_preds_proba.append(y_pred.cpu().numpy())
                val_labels.append(y_batch.cpu().numpy())
        
        val_preds_proba = np.concatenate(val_preds_proba)
        val_labels = np.concatenate(val_labels)
        
        current_score = 0.0
        if task == 'classification':
            val_preds_class = np.argmax(val_preds_proba, axis=1)
            current_score = accuracy_score(val_labels, val_preds_class)
        elif task == 'regression':
            current_score = np.sqrt(mean_squared_error(val_labels, val_preds_proba.squeeze()))
        
        print(f"è¯•éªŒ {i+1} éªŒè¯é›† {scoring_metric}: {current_score:.4f}")
        
        if (mode == 'max' and current_score > best_score) or \
           (mode == 'min' and current_score < best_score):
            best_score = current_score
            best_params = params
            print(f"  (å‘ç°æ–°çš„æœ€ä½³å‚æ•°!)")
            
    print("\n" + "-"*10 + " é˜¶æ®µä¸€æœç´¢å®Œæˆ " + "-"*10)
    print(f"æœ€ä½³éªŒè¯é›† {scoring_metric}: {best_score:.4f}")
    print(f"é€‰å®šçš„æœ€ä½³å‚æ•°: {best_params}")
    return best_params

def train_final_model(best_params, cfg, seed, loaders, model_inputs, device, 
                      patience: int, max_epochs: int):
    print("\n" + "-"*10 + f" [ç§å­ {seed}] é˜¶æ®µäºŒï¼šä½¿ç”¨æ—©åœæœºåˆ¶å……åˆ†è®­ç»ƒæœ€ä½³æ¨¡å‹ " + "-"*10)
    
    train_loader, val_loader = loaders['train'], loaders['val']
    n_num, cats, d_out, task = model_inputs.values()
    
    set_seed(seed)
    model = create_model(best_params, n_num, cats, d_out, device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])
    loss_fn = create_loss_fn(task, device)

    best_val_loss = float('inf')
    patience_counter = 0
    best_model_path = f'best_model_seed_{seed}.pt' 

    for epoch in range(max_epochs):
        model.train()
        for x_num_batch, x_cat_batch, y_batch in train_loader:
            x_num_batch, x_cat_batch, y_batch = x_num_batch.to(device), x_cat_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            y_pred = model(x_num_batch, x_cat_batch)
            
            # [ä¿®æ”¹ 2] ä¿®å¤ Loss ç±»å‹ï¼šå›å½’ç”¨ floatï¼Œåˆ†ç±»ç”¨ long
            target = y_batch.long() if task == 'classification' else y_batch.float()
            loss = loss_fn(y_pred, target)
            
            loss.backward()
            optimizer.step()
        
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for x_num_batch, x_cat_batch, y_batch in val_loader:
                x_num_batch, x_cat_batch, y_batch = x_num_batch.to(device), x_cat_batch.to(device), y_batch.to(device)
                y_pred = model(x_num_batch, x_cat_batch)
                
                target = y_batch.long() if task == 'classification' else y_batch.float()
                val_loss += loss_fn(y_pred, target).item()
                
        val_loss /= len(val_loader)
        print(f"Epoch {epoch + 1}/{max_epochs}, Val Loss: {val_loss:.4f}")

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            torch.save(model.state_dict(), best_model_path)
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"  (éªŒè¯é›†æŸå¤±è¿ç»­ {patience} ä¸ªepochæœªæ”¹å–„ï¼Œè§¦å‘æ—©åœ!)")
                break
    
    print(f"åŠ è½½åœ¨éªŒè¯é›†ä¸Šæ€§èƒ½æœ€ä½³çš„æ¨¡å‹ (æ¥è‡ª {best_model_path})...")
    model.load_state_dict(torch.load(best_model_path))
    if os.path.exists(best_model_path):
        os.remove(best_model_path)
        
    return model

# é˜¶æ®µä¸‰å‡½æ•°ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
def evaluate_final_model(cfg, final_model, test_loader, task, device):
    final_model.eval()
    all_preds_proba, all_labels = [], []
    with torch.no_grad():
        for x_num_batch, x_cat_batch, y_batch in test_loader:
            x_num_batch, x_cat_batch = x_num_batch.to(device), x_cat_batch.to(device)
            
            y_pred = final_model(x_num_batch, x_cat_batch)
            
            if task == 'classification':
                all_preds_proba.append(y_pred.softmax(dim=1).cpu().numpy())
            else:
                all_preds_proba.append(y_pred.cpu().numpy())
            all_labels.append(y_batch.cpu().numpy())
            
    all_preds_proba = np.concatenate(all_preds_proba)
    all_labels = np.concatenate(all_labels)

    metrics_dict = {}
    result_line = ""

    if task == 'classification':
        all_preds_class = np.argmax(all_preds_proba, axis=1)
        acc = accuracy_score(all_labels, all_preds_class)
        if cfg.num_classes == 2:
            auc = roc_auc_score(all_labels, all_preds_proba[:, 1])
        else:
            auc = roc_auc_score(all_labels, all_preds_proba, multi_class='ovr', average='macro')
        macro_f1 = f1_score(all_labels, all_preds_class, average='macro')
        
        metrics_dict = {'acc': acc, 'auc': auc, 'macro-F1': macro_f1}
        result_line = f"acc:{acc:.4f},auc:{auc:.4f},macro-F1:{macro_f1:.4f}"
        
    elif task == 'regression':
        # [ä¿®æ”¹ 3] æ–°å¢ MAE å’Œ R2 è®¡ç®—é€»è¾‘
        preds = all_preds_proba.squeeze() # ç¡®ä¿æ˜¯ä¸€ç»´æ•°ç»„
        
        rmse = np.sqrt(mean_squared_error(all_labels, preds))
        mae = mean_absolute_error(all_labels, preds)
        r2 = r2_score(all_labels, preds)
        
        metrics_dict = {'rmse': rmse, 'mae': mae, 'r2': r2}
        result_line = f"rmse:{rmse:.4f}, mae:{mae:.4f}, r2:{r2:.4f}"

    print(f"æœ€ç»ˆæµ‹è¯•é›†æ€§èƒ½: {result_line}")
    return metrics_dict, result_line

# --- 6. ä¸»æ‰§è¡Œæµç¨‹ ---
@hydra.main(version_base=None, config_path="../configs", config_name="config")
def main(cfg: DictConfig):

    print("--- 1.A. æ­£åœ¨è§£ææ•°æ®è·¯å¾„ ---")
    data_root = cfg.get('data_base')

    if data_root is not None:
        print(f"    æ£€æµ‹åˆ° 'data_root'ï¼Œå°†ä¸ºæ‰€æœ‰æ•°æ®æ–‡ä»¶æ·»åŠ å‰ç¼€: {data_root}")
        path_keys = [
            'labels_train', 'labels_val',
            'data_train_imaging', 'data_val_imaging',
            'data_train_tabular', 'data_val_tabular',
            'field_lengths_tabular',
            'data_train_eval_tabular', 'labels_train_eval_tabular',
            'data_val_eval_tabular', 'labels_val_eval_tabular',
            'data_test_eval_tabular', 'labels_test_eval_tabular',
            'data_train_eval_imaging', 'labels_train_eval_imaging',
            'data_val_eval_imaging', 'labels_val_eval_imaging',
            'data_test_eval_imaging', 'labels_test_eval_imaging'
        ]
        for key in path_keys:
            if key in cfg and cfg[key] is not None:
                original_path = cfg[key]
                absolute_path = os.path.join(data_root, original_path)
                cfg[key] = absolute_path 
            
    else:
        print("    æœªæä¾› 'data_root'ã€‚å°†å‡å®š config ä¸­çš„è·¯å¾„å·²ç»æ˜¯æ­£ç¡®çš„ã€‚")
    
    print("--- æœ€ç»ˆé…ç½®: ---")
    print(OmegaConf.to_yaml(cfg))
    print("--------------------")
    print(f"Hydra å·¥ä½œç›®å½•: {os.getcwd()}")
    print("--------------------")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'æ­£åœ¨ä½¿ç”¨è®¾å¤‡: {device}')
    
    original_cwd = get_original_cwd() 
    model_config_filename = 'ftt_model_config.json'
    model_config_path = os.path.join(original_cwd, model_config_filename)
    
    print(f"--- æ­£åœ¨ä» {model_config_path} åŠ è½½æ¨¡å‹é…ç½® ---")
    try:
        with open(model_config_path, 'r') as f:
            model_config = json.load(f)
        search_space = model_config['search_space']
        hyperparams = model_config['hyperparams']
        print("æ¨¡å‹é…ç½®åŠ è½½æˆåŠŸã€‚")
    except Exception as e:
        print(f"é”™è¯¯: åŠ è½½æ¨¡å‹é…ç½®å¤±è´¥: {e}")
        sys.exit(1)
    
    seeds = list(hyperparams['seeds'])
    batch_size = hyperparams['batch_size']
    D_TOKEN = hyperparams['d_token']
    LEARNING_RATE = hyperparams['learning_rate']
    WEIGHT_DECAY = hyperparams['weight_decay']
    N_TRIALS = hyperparams['n_trials']
    patience = hyperparams['patience']
    max_epochs = hyperparams['max_epochs']

    final_results_summary = []

    loaders, model_inputs = load_and_preprocess_data(cfg, batch_size)

    for seed in seeds:
        print("\n" + "="*30 + f" å¼€å§‹æ‰§è¡Œï¼Œéšæœºç§å­: {seed} " + "="*30)
        set_seed(seed)

        param_combinations = []
        for _ in range(N_TRIALS):
            params = {
                'n_blocks': random.choice(search_space['n_blocks']),
                'ffn_d_hidden': random.choice(search_space['ffn_d_hidden']),
                'residual_dropout': random.uniform(*search_space['residual_dropout']),
                'attention_dropout': random.uniform(*search_space['attention_dropout']),
                'ffn_dropout': random.uniform(*search_space['ffn_dropout']),
                'd_token': D_TOKEN, 
                'learning_rate': LEARNING_RATE, 
                'weight_decay': WEIGHT_DECAY,
            }
            param_combinations.append(params)
        
        best_params = search_for_best_params(
            param_combinations, cfg, seed, loaders, model_inputs, device
        )
        
        final_model = train_final_model(
            best_params, cfg, seed, loaders, model_inputs, device,
            patience=patience, max_epochs=max_epochs
        )
        
        print("\n" + "-"*10 + f" [ç§å­ {seed}] é˜¶æ®µä¸‰ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼° " + "-"*10)
        # [ä¿®æ”¹ 4] ä¿®å¤å‡½æ•°è°ƒç”¨ï¼Œä¼ å…¥ cfg å‚æ•°
        metrics_dict, result_line = evaluate_final_model(
            cfg, final_model, loaders['test'], model_inputs['task'], device
        )
        
        result_dict = {
            'seed': seed, 
            'best_params': best_params, 
            'result_line': result_line,
            **metrics_dict
        }
        final_results_summary.append(result_dict)

    print("\n\n" + "="*30 + " æ‰€æœ‰å®éªŒæœ€ç»ˆæ€»ç»“ " + "="*30)
    
    output_file_path = 'result/fttrans.txt'
    print(f"å‡†å¤‡å°†ç»“æœå†™å…¥åˆ°: {output_file_path}")

    output_dir = os.path.dirname(output_file_path)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    with open(output_file_path, 'a') as f:
        f.write(f"--- å®éªŒé…ç½® (æ¥è‡ª config.yaml): {cfg.target} ---\n")
        f.write(OmegaConf.to_yaml(cfg)) 
        f.write("\n--- æ¨¡å‹é…ç½® (æ¥è‡ª ftt_model_config.json) ---\n")
        f.write(json.dumps(model_config, indent=2))
        f.write("\n\n" + "="*30 + " æ‰€æœ‰å®éªŒæœ€ç»ˆæ€»ç»“ " + "="*30 + "\n")
        
        for final_result in final_results_summary:
            result_line = f"ç§å­: {final_result['seed']} | {final_result['result_line']}"
            print(result_line)
            f.write(result_line + "\n")

        params_header = f"\næœ€ä½³å‚æ•°çš„ä¾‹å­ (æ¥è‡ªæœ€åä¸€ä¸ªç§å­ {final_results_summary[-1]['seed']}):"
        params_details = str(final_results_summary[-1]['best_params'])
        
        print(params_header)
        print(params_details)
        f.write(params_header + "\n")
        f.write(params_details + "\n")
        f.write("="*80 + "\n")

    print(f"\nç»“æœå·²æˆåŠŸå†™å…¥åˆ°æ–‡ä»¶: {output_file_path}")


if __name__ == "__main__":
    main()