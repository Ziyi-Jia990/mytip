import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import pandas as pd
import numpy as np
import rtdl
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, mean_squared_error
import time
import os
import random
import sys
import json

# --- Hydra å¯¼å…¥ ---
import hydra
from omegaconf import DictConfig, OmegaConf
from hydra.utils import get_original_cwd, to_absolute_path # 

# --- 0. é…ç½®ä¸è®¾ç½®éšæœºç§å­ ---
def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# --- è‡ªå®šä¹‰æ¨¡å‹ç±» ---
class MyFTTransformer(nn.Module):
    def __init__(self, ft_transformer_module):
        super().__init__()
        self.ft_transformer = ft_transformer_module

    def forward(self, x_num, x_cat):
        if x_num is not None and x_num.shape[1] == 0:
            x_num = None
            
        out = self.ft_transformer(x_num, x_cat)

        if isinstance(out, tuple):
            x_embed, x_out = out   
        else:
            x_out = out           

        # å›å½’ä»»åŠ¡ d_out=1 æ—¶ï¼Œåšä¸ª squeeze
        if x_out.shape[-1] == 1:
            return x_out.squeeze(-1)
        return x_out


# --- 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† (é‡æ„ä¸ºå‡½æ•°) ---
def load_and_preprocess_data(cfg: DictConfig, batch_size: int):
    print(f"--- 1. æ­£åœ¨ä¸ºæ•°æ®é›†: '{cfg.target}' (é€šç”¨åŠ è½½å™¨) åŠ è½½æ•°æ® ---")

    try:
        # --- 1. åŠ è½½å­—æ®µé•¿åº¦ (åŸºæ•°) ---
        # ä½ çš„é€»è¾‘ï¼š == 1 æ˜¯è¿ç»­ç‰¹å¾ï¼Œ > 1 æ˜¯ç±»åˆ«ç‰¹å¾
        all_field_lengths = torch.load(cfg.field_lengths_tabular)
        
        # è½¬æ¢ä¸º list æˆ– numpy æ–¹ä¾¿å¤„ç†
        if isinstance(all_field_lengths, torch.Tensor):
            all_field_lengths = all_field_lengths.cpu().tolist()
            
        # === æ ¸å¿ƒä¿®æ”¹ï¼šåŠ¨æ€è¯†åˆ«ç´¢å¼• ===
        con_indices = [i for i, length in enumerate(all_field_lengths) if length == 1]
        cat_indices = [i for i, length in enumerate(all_field_lengths) if length > 1]
        
        num_con = len(con_indices)
        num_cat = len(cat_indices)
        
        # æå–åˆ†ç±»ç‰¹å¾çš„åŸºæ•° (Cardinalities)ï¼Œé¡ºåºå¿…é¡»ä¸ cat_indices å¯¹åº”
        cat_cardinalities = [all_field_lengths[i] for i in cat_indices]

        print(f"    è‡ªåŠ¨æ£€æµ‹ç»“æœ: {num_con} ä¸ªè¿ç»­ç‰¹å¾, {num_cat} ä¸ªåˆ†ç±»ç‰¹å¾ã€‚")

        # --- 2. åŠ è½½ç‰¹å¾ (CSV) ---
        train_df = pd.read_csv(cfg.data_train_eval_tabular, header=None)
        val_df = pd.read_csv(cfg.data_val_eval_tabular, header=None)
        test_df = pd.read_csv(cfg.data_test_eval_tabular, header=None)

        # ç®€å•çš„æ ¡éªŒ
        if train_df.shape[1] != len(all_field_lengths):
            print(f"ğŸ”´ é”™è¯¯ï¼šCSV åˆ—æ•° ({train_df.shape[1]}) ä¸ field_lengths é•¿åº¦ ({len(all_field_lengths)}) ä¸ä¸€è‡´ï¼")
            sys.exit(1)

        # --- 3. åŠ è½½æ ‡ç­¾ ---
        y_train = torch.load(cfg.labels_train_eval_tabular)
        y_val = torch.load(cfg.labels_val_eval_tabular)
        y_test = torch.load(cfg.labels_test_eval_tabular)

    except Exception as e:
        print(f"ğŸ”´ åŠ è½½æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)

    # --- 4. æ‹†åˆ†ç‰¹å¾å¹¶è½¬æ¢ä¸º Tensors (ä¿®æ”¹ç‰ˆ) ---
    
    def split_and_convert_to_tensors(df, y_tensor):
        # === ä½¿ç”¨ç´¢å¼•æå–æ•°æ® ===
        # å³ä½¿ con_indices ä¸ºç©ºï¼Œdf.iloc[:, []] ä¹Ÿä¼šè¿”å›ç©º DFï¼Œä¸ä¼šæŠ¥é”™
        X_num_df = df.iloc[:, con_indices]
        X_cat_df = df.iloc[:, cat_indices]
        
        # === å…³é”®ä¿®å¤ï¼šè§£å†³ rtdl æŠ¥é”™ ===
        # å¦‚æœæ²¡æœ‰æ•°å€¼ç‰¹å¾ï¼Œç›´æ¥è®¾ä¸º Noneï¼Œè€Œä¸æ˜¯ç©º Tensor
        if len(con_indices) > 0:
            X_num_tensor = torch.tensor(X_num_df.values.astype(np.float32))
        else:
            X_num_tensor = None  # <--- è¿™é‡Œç›´æ¥ç»™ Noneï¼Œè§£å†³ä¹‹å‰çš„ AssertionError

        # ç±»åˆ«ç‰¹å¾å¤„ç†
        X_cat_tensor = torch.tensor(X_cat_df.values.astype(np.int64))
        
        # æ ‡ç­¾å¤„ç†
        if cfg.task == 'classification':
            y_tensor = y_tensor.long()
        else:
            y_tensor = y_tensor.float()

        return X_num_tensor, X_cat_tensor, y_tensor

    print("    æ­£åœ¨æ ¹æ® field_lengths æ‹†åˆ†å¹¶è½¬æ¢æ•°æ®...")
    # è¿™é‡ŒåŸæ¥çš„ Dataset å¯èƒ½ä¼šæŠ¥é”™ï¼Œå› ä¸º TensorDataset ä¸æ”¯æŒ None
    # æˆ‘ä»¬éœ€è¦ä¸‹é¢ç‰¹åˆ«å¤„ç† TensorDataset
    
    X_train_num, X_train_cat, y_train = split_and_convert_to_tensors(train_df, y_train)
    X_val_num, X_val_cat, y_val = split_and_convert_to_tensors(val_df, y_val)
    X_test_num, X_test_cat, y_test = split_and_convert_to_tensors(test_df, y_test)
    
    # ==================================================================
    # == åˆ›å»º DataLoaders (éœ€è¦å¤„ç† None çš„æƒ…å†µ)
    # ==================================================================
    
    # å®šä¹‰ä¸€ä¸ªå®‰å…¨çš„ Dataset ç±»ï¼Œå…è®¸ x_num ä¸º None
    class SafeTabularDataset(torch.utils.data.Dataset):
        def __init__(self, x_num, x_cat, y):
            self.x_num = x_num
            self.x_cat = x_cat
            self.y = y
            
        def __len__(self):
            return len(self.y)
            
        def __getitem__(self, idx):
            # å¦‚æœ x_num æ˜¯ Noneï¼Œè¿”å›ä¸€ä¸ªå ä½ç¬¦æˆ–è€…åœ¨ collate_fn é‡Œå¤„ç†
            # ç®€å•èµ·è§ï¼Œè¿™é‡Œæˆ‘ä»¬è¿”å›ä¸€ä¸ªç©ºçš„ tensor (å¦‚æœæ˜¯None)ï¼Œ
            # ä½†æ—¢ç„¶æˆ‘ä»¬ä¸Šé¢ä¸ºäº†è§£å†³ rtdl æ”¹æˆäº† Noneï¼Œè¿™é‡Œä¸ºäº† DataLoader æ–¹ä¾¿ï¼Œ
            # æˆ‘ä»¬å¯ä»¥ä¿ç•™ Noneï¼Œä½†åœ¨å–å‡ºæ—¶è¦æ³¨æ„ã€‚
            
            # æ›´åŠ ç®€ä¾¿çš„æ–¹æ³•ï¼š
            # å¦‚æœ x_num æ˜¯ Noneï¼Œæˆ‘ä»¬å°±ä¸æŠŠå®ƒæ”¾è¿› TensorDatasetï¼Œè€Œæ˜¯é€ ä¸€ä¸ªè‡ªå®šä¹‰ Dataset
            num_val = self.x_num[idx] if self.x_num is not None else torch.empty(0)
            return num_val, self.x_cat[idx], self.y[idx]

    # ä¸ºäº†ä¸å¼•å…¥å¤æ‚çš„ Dataset ç±»ï¼Œæœ€ç®€å•çš„ Hack æ–¹æ³•ï¼š
    # å¦‚æœ x_num æ˜¯ Noneï¼Œæˆ‘ä»¬è¿˜æ˜¯å˜å›ç©º Tensor å­˜å…¥ DataLoaderï¼Œ
    # ä½†æ˜¯åœ¨ Model çš„ forward é‡Œè¿›è¡Œåˆ¤æ–­ï¼ˆå°±åƒä¸Šä¸€æ¡å›å¤å»ºè®®çš„é‚£æ ·ï¼‰ã€‚
    
    # ä¿®æ­£ç­–ç•¥ï¼š
    # 1. è¿™é‡Œ DataLoader é‡Œè¿˜æ˜¯å­˜ç©º Tensor (ä¸ºäº†æ–¹ä¾¿æ‰¹å¤„ç†)
    # 2. Model é‡ŒåŠ åˆ¤æ–­ (ä¸Šä¸€æ¡å›å¤çš„æ–¹æ¡ˆ)
    # è¿™æ ·æ”¹åŠ¨æœ€å°ã€‚
    
    # é‡æ–°ä¿®æ­£ split_and_convert_to_tensors çš„è¿”å›å€¼ï¼Œæ”¹å›è¿”å› Tensor
    if X_train_num is None: X_train_num = torch.empty((len(y_train), 0))
    if X_val_num is None: X_val_num = torch.empty((len(y_val), 0))
    if X_test_num is None: X_test_num = torch.empty((len(y_test), 0))

    try:
        train_dataset = TensorDataset(X_train_num, X_train_cat, y_train)
        val_dataset = TensorDataset(X_val_num, X_val_cat, y_val)
        test_dataset = TensorDataset(X_test_num, X_test_cat, y_test)
    except Exception as e:
        print(f"ğŸ”´ åˆ›å»º Dataset å‡ºé”™: {e}")
        sys.exit(1)
        
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # --- 3. å‡†å¤‡æ¨¡å‹è¾“å…¥ ---
    
    # æ¨¡å‹è¾“å‡ºç»´åº¦
    d_out = cfg.num_classes
    
    # è¿”å›æ‰€æœ‰å¿…è¦ç»„ä»¶
    model_inputs = {
        "n_num_features": num_con,          # æ•°å€¼ç‰¹å¾çš„æ•°é‡
        "cat_cardinalities": cat_cardinalities, # åˆ†ç±»ç‰¹å¾åŸºæ•°åˆ—è¡¨
        "d_out": d_out,
        "task": cfg.task
    }
    loaders = {
        "train": train_loader,
        "val": val_loader,
        "test": test_loader
    }
    
    return loaders, model_inputs

# --- 3. å®šä¹‰æ¨¡å‹åˆ›å»ºå‡½æ•° (å·²ä¿®æ­£ï¼šè¡¥å…¨ rtdl.Transformer æ‰€éœ€çš„å‚æ•°) ---
def create_model(params, n_num_features, cat_cardinalities, d_out, device):
    base_ft_transformer = rtdl.FTTransformer(
        feature_tokenizer=rtdl.modules.FeatureTokenizer(
            n_num_features=n_num_features, 
            cat_cardinalities=cat_cardinalities, 
            d_token=params['d_token']
        ),
        transformer=rtdl.modules.Transformer(
            d_token=params['d_token'],
            n_blocks=params['n_blocks'],
            attention_dropout=params['attention_dropout'],
            ffn_d_hidden=params['ffn_d_hidden'],
            ffn_dropout=params['ffn_dropout'],
            residual_dropout=params['residual_dropout'],

            attention_n_heads=8,
            attention_initialization='kaiming',
            attention_normalization='LayerNorm',
            ffn_activation='ReLU',
            ffn_normalization='LayerNorm',
            prenormalization=True,
            first_prenormalization=False,
            last_layer_query_idx=[-1],
            n_tokens=None,
            kv_compression_ratio=None,
            kv_compression_sharing=None,
            head_activation=nn.Identity,
            head_normalization=nn.Identity,

            # ğŸ”´ å…³é”®ï¼šè¿™é‡Œçš„ d_out ä¸€å®šè¦æ˜¯ â€œæœ€ç»ˆè¾“å‡ºç»´åº¦â€ï¼Œä¹Ÿå°±æ˜¯ num_classes
            d_out=d_out,
        ),
    )

    model = MyFTTransformer(
        ft_transformer_module=base_ft_transformer,
    ).to(device)

    return model


# --- 4. è¾…åŠ©å‡½æ•°ï¼šè·å–æŸå¤±å‡½æ•°å’Œè¯„ä¼°æŒ‡æ ‡ ---
def create_loss_fn(task, device):
    if task == 'classification':
        return nn.CrossEntropyLoss().to(device)
    elif task == 'regression':
        return nn.MSELoss().to(device)
    else:
        raise ValueError(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task}")

def get_scoring_info(task):
    """è·å–é˜¶æ®µä¸€æœç´¢æ‰€éœ€çš„è¯„ä¼°æŒ‡æ ‡å’Œä¼˜åŒ–æ–¹å‘"""
    if task == 'classification':
        return 'accuracy', 'max' # æŒ‡æ ‡åç§°, ä¼˜åŒ–æ–¹å‘
    elif task == 'regression':
        return 'rmse', 'min'
    else:
        raise ValueError(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task}")

# --- 5. å®šä¹‰è®­ç»ƒä¸è¯„ä¼°å‡½æ•° ---

# é˜¶æ®µä¸€å‡½æ•°ï¼šå¿«é€Ÿæœç´¢
def search_for_best_params(param_combinations, cfg, seed, loaders, model_inputs, device):
    print("\n" + "-"*10 + f" [ç§å­ {seed}] é˜¶æ®µä¸€ï¼šå¿«é€Ÿè¶…å‚æ•°æœç´¢ (15 epochs) " + "-"*10)
    
    train_loader, val_loader = loaders['train'], loaders['val']
    n_num, cats, d_out, task = model_inputs.values()
    
    scoring_metric, mode = get_scoring_info(task)
    best_score = -float('inf') if mode == 'max' else float('inf')
    best_params = None
    
    for i, params in enumerate(param_combinations):
        print(f"\n--- [è¯•éªŒ {i+1}/{len(param_combinations)}] ---")
        print(f"æµ‹è¯•å‚æ•°: {params}")
        
        set_seed(seed)
        model = create_model(params, n_num, cats, d_out, device)
        optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])
        loss_fn = create_loss_fn(task, device)
        
        # è®­ç»ƒå›ºå®šçš„15ä¸ªepoch
        for epoch in range(15):
            model.train()
            for x_num_batch, x_cat_batch, y_batch in train_loader:
                x_num_batch, x_cat_batch, y_batch = x_num_batch.to(device), x_cat_batch.to(device), y_batch.to(device)
                optimizer.zero_grad()
                y_pred = model(x_num_batch, x_cat_batch)
                # print("y_pred shape:", y_pred.shape, "dtype:", y_pred.dtype)
                # print("y_batch shape:", y_batch.shape, "dtype:", y_batch.dtype)

                loss = loss_fn(y_pred, y_batch.long())
                loss.backward()
                optimizer.step()
                

        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        model.eval()
        val_preds_proba = []
        val_labels = []
        with torch.no_grad():
            for x_num_batch, x_cat_batch, y_batch in val_loader:
                x_num_batch, x_cat_batch = x_num_batch.to(device), x_cat_batch.to(device)
                y_pred = model(x_num_batch, x_cat_batch)
                
                # ç»Ÿä¸€å¤„ç†: proba ç”¨äºåˆ†ç±», value ç”¨äºå›å½’
                if task == 'classification':
                    val_preds_proba.append(y_pred.softmax(dim=1).cpu().numpy())
                else:
                    val_preds_proba.append(y_pred.cpu().numpy()) # (N,) or (N, 1)
                val_labels.append(y_batch.cpu().numpy())
        
        val_preds_proba = np.concatenate(val_preds_proba)
        val_labels = np.concatenate(val_labels)
        
        # åŠ¨æ€è®¡ç®—å¾—åˆ†
        current_score = 0.0
        if task == 'classification':
            val_preds_class = np.argmax(val_preds_proba, axis=1)
            current_score = accuracy_score(val_labels, val_preds_class)
        elif task == 'regression':
            current_score = np.sqrt(mean_squared_error(val_labels, val_preds_proba.squeeze()))
        
        print(f"è¯•éªŒ {i+1} éªŒè¯é›† {scoring_metric}: {current_score:.4f}")
        
        if (mode == 'max' and current_score > best_score) or \
           (mode == 'min' and current_score < best_score):
            best_score = current_score
            best_params = params
            print(f"  (å‘ç°æ–°çš„æœ€ä½³å‚æ•°!)")
            
    print("\n" + "-"*10 + " é˜¶æ®µä¸€æœç´¢å®Œæˆ " + "-"*10)
    print(f"æœ€ä½³éªŒè¯é›† {scoring_metric}: {best_score:.4f}")
    print(f"é€‰å®šçš„æœ€ä½³å‚æ•°: {best_params}")
    return best_params

# é˜¶æ®µäºŒå‡½æ•°ï¼šä½¿ç”¨æ—©åœå……åˆ†è®­ç»ƒ
def train_final_model(best_params, cfg, seed, loaders, model_inputs, device, 
                      patience: int, max_epochs: int):
    print("\n" + "-"*10 + f" [ç§å­ {seed}] é˜¶æ®µäºŒï¼šä½¿ç”¨æ—©åœæœºåˆ¶å……åˆ†è®­ç»ƒæœ€ä½³æ¨¡å‹ " + "-"*10)
    
    train_loader, val_loader = loaders['train'], loaders['val']
    n_num, cats, d_out, task = model_inputs.values()
    
    set_seed(seed)
    model = create_model(best_params, n_num, cats, d_out, device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])
    loss_fn = create_loss_fn(task, device)

    best_val_loss = float('inf')
    patience_counter = 0
    best_model_path = f'best_model_seed_{seed}.pt' 
    # [!] ä½¿ç”¨æ¥è‡ª json çš„å‚æ•°
    # max_epochs = cfg.hyperparams.max_epochs (ç§»é™¤)
    # patience = cfg.hyperparams.patience (ç§»é™¤)

    for epoch in range(max_epochs):
        model.train()
        for x_num_batch, x_cat_batch, y_batch in train_loader:
            x_num_batch, x_cat_batch, y_batch = x_num_batch.to(device), x_cat_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            y_pred = model(x_num_batch, x_cat_batch)
            loss = loss_fn(y_pred, y_batch.long())
            loss.backward()
            optimizer.step()
        
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for x_num_batch, x_cat_batch, y_batch in val_loader:
                x_num_batch, x_cat_batch, y_batch = x_num_batch.to(device), x_cat_batch.to(device), y_batch.to(device)
                y_pred = model(x_num_batch, x_cat_batch)
                val_loss += loss_fn(y_pred, y_batch.long()).item()
        val_loss /= len(val_loader)
        print(f"Epoch {epoch + 1}/{max_epochs}, Val Loss: {val_loss:.4f}")

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            torch.save(model.state_dict(), best_model_path)
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"  (éªŒè¯é›†æŸå¤±è¿ç»­ {patience} ä¸ªepochæœªæ”¹å–„ï¼Œè§¦å‘æ—©åœ!)")
                break
    
    print(f"åŠ è½½åœ¨éªŒè¯é›†ä¸Šæ€§èƒ½æœ€ä½³çš„æ¨¡å‹ (æ¥è‡ª {best_model_path})...")
    model.load_state_dict(torch.load(best_model_path))
    if os.path.exists(best_model_path):
        os.remove(best_model_path)
        
    return model

# é˜¶æ®µä¸‰å‡½æ•°ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
def evaluate_final_model(cfg, final_model, test_loader, task, device):
    final_model.eval()
    all_preds_proba, all_labels = [], []
    with torch.no_grad():
        for x_num_batch, x_cat_batch, y_batch in test_loader:
            x_num_batch, x_cat_batch = x_num_batch.to(device), x_cat_batch.to(device)
            
            y_pred = final_model(x_num_batch, x_cat_batch)
            
            if task == 'classification':
                all_preds_proba.append(y_pred.softmax(dim=1).cpu().numpy())
            else:
                all_preds_proba.append(y_pred.cpu().numpy())
            all_labels.append(y_batch.cpu().numpy())
            
    all_preds_proba = np.concatenate(all_preds_proba)
    all_labels = np.concatenate(all_labels)

    metrics_dict = {}
    result_line = ""

    if task == 'classification':
        all_preds_class = np.argmax(all_preds_proba, axis=1)
        acc = accuracy_score(all_labels, all_preds_class)
        # auc = roc_auc_score(all_labels, all_preds_proba, multi_class='ovr', average='macro')
        if cfg.num_classes == 2:
            auc = roc_auc_score(all_labels, all_preds_proba[:, 1])
        else:
            # å¦‚æœæœªæ¥æœ‰å¤šåˆ†ç±»éœ€æ±‚ï¼Œä¿ç•™åŸé€»è¾‘
            auc = roc_auc_score(all_labels, all_preds_proba, multi_class='ovr', average='macro')
        macro_f1 = f1_score(all_labels, all_preds_class, average='macro')
        
        metrics_dict = {'acc': acc, 'auc': auc, 'macro-F1': macro_f1}
        result_line = f"acc:{acc:.4f},auc:{auc:.4f},macro-F1:{macro_f1:.4f}"
        
    elif task == 'regression':
        rmse = np.sqrt(mean_squared_error(all_labels, all_preds_proba.squeeze()))
        metrics_dict = {'rmse': rmse}
        result_line = f"rmse:{rmse:.4f}"

    print(f"æœ€ç»ˆæµ‹è¯•é›†æ€§èƒ½: {result_line}")
    return metrics_dict, result_line

# --- 6. ä¸»æ‰§è¡Œæµç¨‹ (Hydra Main) ---
@hydra.main(version_base=None, config_path="../configs", config_name="config")
def main(cfg: DictConfig):

    print("--- 1.A. æ­£åœ¨è§£ææ•°æ®è·¯å¾„ ---")
    
    # 1. ä»å‘½ä»¤è¡Œè·å– 'data_root'
    #    (å¦‚æœæœªæä¾›ï¼Œdata_root å°†ä¸º None)
    data_root = cfg.get('data_base')

    if data_root is not None:
        print(f"    æ£€æµ‹åˆ° 'data_root'ï¼Œå°†ä¸ºæ‰€æœ‰æ•°æ®æ–‡ä»¶æ·»åŠ å‰ç¼€: {data_root}")
        
        # 2. å®šä¹‰åœ¨ .yaml ä¸­æ‰€æœ‰â€œéœ€è¦â€æ·»åŠ å‰ç¼€çš„è·¯å¾„é”®
        path_keys = [
            'labels_train', 'labels_val',
            'data_train_imaging', 'data_val_imaging',
            'data_train_tabular', 'data_val_tabular',
            'field_lengths_tabular',
            'data_train_eval_tabular', 'labels_train_eval_tabular',
            'data_val_eval_tabular', 'labels_val_eval_tabular',
            'data_test_eval_tabular', 'labels_test_eval_tabular',
            'data_train_eval_imaging', 'labels_train_eval_imaging',
            'data_val_eval_imaging', 'labels_val_eval_imaging',
            'data_test_eval_imaging', 'labels_test_eval_imaging'
        ]
        
        # 3. éå†è¿™äº›é”®ï¼Œå¦‚æœå®ƒä»¬å­˜åœ¨äº cfg ä¸­ï¼Œåˆ™æ›´æ–°è·¯å¾„
        for key in path_keys:
            if key in cfg and cfg[key] is not None:
                original_path = cfg[key]
                absolute_path = os.path.join(data_root, original_path)
                cfg[key] = absolute_path # [!] ç›´æ¥ä¿®æ”¹ config å¯¹è±¡
                # print(f"        {key}: {original_path} -> {absolute_path}") # (å¯é€‰ï¼šç”¨äºè°ƒè¯•)
            
    else:
        print("    æœªæä¾› 'data_root'ã€‚å°†å‡å®š config ä¸­çš„è·¯å¾„å·²ç»æ˜¯æ­£ç¡®çš„ (ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹äºCWD)ã€‚")
    
    print("--- æœ€ç»ˆé…ç½®: ---")
    print(OmegaConf.to_yaml(cfg))
    print("--------------------")
    print(f"Hydra å·¥ä½œç›®å½•: {os.getcwd()}")
    print("--------------------")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'æ­£åœ¨ä½¿ç”¨è®¾å¤‡: {device}')
    
    original_cwd = get_original_cwd() 
    model_config_filename = 'ftt_model_config.json' # [!] æ–‡ä»¶åä¿®æ”¹
    model_config_path = os.path.join(original_cwd, model_config_filename)
    
    print(f"--- æ­£åœ¨ä» {model_config_path} åŠ è½½æ¨¡å‹é…ç½® ---")
    try:
        with open(model_config_path, 'r') as f:
            model_config = json.load(f)
        search_space = model_config['search_space']
        hyperparams = model_config['hyperparams']
        print("æ¨¡å‹é…ç½® (hyperparams å’Œ search_space) åŠ è½½æˆåŠŸã€‚")
    except FileNotFoundError:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹é…ç½®æ–‡ä»¶: {model_config_path}")
        sys.exit(1)
    except KeyError as e:
        print(f"é”™è¯¯: æ¨¡å‹é…ç½®æ–‡ä»¶ '{model_config_path}' ç¼ºå°‘é”®: {e}")
        sys.exit(1)
    
    
    # 2. å°† hyperparams æå–åˆ°å˜é‡
    seeds = list(hyperparams['seeds'])
    batch_size = hyperparams['batch_size']
    D_TOKEN = hyperparams['d_token']
    LEARNING_RATE = hyperparams['learning_rate']
    WEIGHT_DECAY = hyperparams['weight_decay']
    N_TRIALS = hyperparams['n_trials']
    patience = hyperparams['patience']
    max_epochs = hyperparams['max_epochs']

    final_results_summary = []

    # 3. åŠ è½½æ•°æ®, ä¼ å…¥ batch_size
    loaders, model_inputs = load_and_preprocess_data(cfg, batch_size)

    # for seed in seeds:
    #     print("\n" + "="*30 + f" å¼€å§‹æ‰§è¡Œï¼Œéšæœºç§å­: {seed} " + "="*30)
    #     set_seed(seed)

    #     # 4. ç”Ÿæˆéšæœºè¶…å‚æ•°ç»„åˆ (ä½¿ç”¨æ¥è‡ª JSON çš„ search_space)
    #     param_combinations = []
    #     for _ in range(N_TRIALS):
    #         params = {
    #             'n_blocks': random.choice(search_space['n_blocks']),
    #             'ffn_d_hidden': random.choice(search_space['ffn_d_hidden']),
    #             'residual_dropout': random.uniform(*search_space['residual_dropout']),
    #             'attention_dropout': random.uniform(*search_space['attention_dropout']),
    #             'ffn_dropout': random.uniform(*search_space['ffn_dropout']),
    #             'd_token': D_TOKEN, 
    #             'learning_rate': LEARNING_RATE, 
    #             'weight_decay': WEIGHT_DECAY,
    #         }
    #         param_combinations.append(params)
        
    #     # 5. é˜¶æ®µä¸€ï¼šæœç´¢
    #     best_params = search_for_best_params(
    #         param_combinations, cfg, seed, loaders, model_inputs, device
    #     )
        
    #     # [!] 6. é˜¶æ®µäºŒï¼šè®­ç»ƒ, ä¼ å…¥ patience å’Œ max_epochs
    #     final_model = train_final_model(
    #         best_params, cfg, seed, loaders, model_inputs, device,
    #         patience=patience, max_epochs=max_epochs
    #     )
        
    #     # 7. é˜¶æ®µä¸‰ï¼šè¯„ä¼°
    #     print("\n" + "-"*10 + f" [ç§å­ {seed}] é˜¶æ®µä¸‰ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼° " + "-"*10)
    #     metrics_dict, result_line = evaluate_final_model(
    #         final_model, loaders['test'], model_inputs['task'], device
    #     )
        
    #     result_dict = {
    #         'seed': seed, 
    #         'best_params': best_params, 
    #         'result_line': result_line,
    #         **metrics_dict
    #     }
    #     final_results_summary.append(result_dict)

    manual_best_params = {
        'n_blocks': 3, 
        'ffn_d_hidden': 64, 
        'residual_dropout': 0.181130119829332, 
        'attention_dropout': 0.437874063436074, 
        'ffn_dropout': 0.18396799001208675, 
        'd_token': 192, 
        'learning_rate': 0.0001, 
        'weight_decay': 1e-05
    }
    
    print("!!! æ£€æµ‹åˆ°æ‰‹åŠ¨æ¨¡å¼ï¼šè·³è¿‡é˜¶æ®µä¸€æœç´¢ï¼Œç›´æ¥ä½¿ç”¨å·²çŸ¥æœ€ä½³å‚æ•°è¿›è¡Œè®­ç»ƒ !!!")

    for seed in seeds:
        print("\n" + "="*30 + f" å¼€å§‹æ‰§è¡Œï¼Œéšæœºç§å­: {seed} " + "="*30)
        set_seed(seed)

        # ğŸ”´ 2. æ³¨é‡Šæ‰é˜¶æ®µä¸€æœç´¢ä»£ç 
        # param_combinations = ...
        # best_params = search_for_best_params(...)
        
        # ç›´æ¥èµ‹å€¼
        best_params = manual_best_params

        # ğŸ”´ 3. æ‰§è¡Œé˜¶æ®µäºŒ (è¿™ä¸ªå¿…é¡»é‡è·‘ï¼Œå› ä¸ºæ¨¡å‹æ–‡ä»¶ä¹‹å‰è¢«è‡ªåŠ¨åˆ é™¤äº†)
        # è¿™æ¬¡è·‘ä¼šå¾ˆå¿«ï¼Œå› ä¸ºåªéœ€è¦è®­ç»ƒè¿™ä¸€ä¸ªæ¨¡å‹ï¼Œè€Œä¸”æ ¹æ®æ—¥å¿—å®ƒåœ¨ç¬¬10ä¸ªepochå°±æ—©åœäº†
        final_model = train_final_model(
            best_params, cfg, seed, loaders, model_inputs, device,
            patience=patience, max_epochs=max_epochs
        )
        
        # ğŸ”´ 4. æ‰§è¡Œé˜¶æ®µä¸‰ (è¿™é‡Œæ˜¯ä½ æŠ¥é”™çš„åœ°æ–¹ï¼Œç°åœ¨å·²ç»ä¿®å¤äº†)
        print("\n" + "-"*10 + f" [ç§å­ {seed}] é˜¶æ®µä¸‰ï¼šåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼° " + "-"*10)
        metrics_dict, result_line = evaluate_final_model(
            cfg, final_model, loaders['test'], model_inputs['task'], device
        )
        
        result_dict = {
            'seed': seed, 
            'best_params': best_params, 
            'result_line': result_line,
            **metrics_dict
        }
        final_results_summary.append(result_dict)

    # --- 7. æœ€ç»ˆæ€»ç»“ ---
    print("\n\n" + "="*30 + " æ‰€æœ‰å®éªŒæœ€ç»ˆæ€»ç»“ " + "="*30)
    
    output_file_path = 'result/fttrans.txt'
    print(f"å‡†å¤‡å°†ç»“æœå†™å…¥åˆ°: {output_file_path}")

    output_dir = os.path.dirname(output_file_path)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    with open(output_file_path, 'a') as f:
        f.write(f"--- å®éªŒé…ç½® (æ¥è‡ª config.yaml): {cfg.target} ---\n")
        f.write(OmegaConf.to_yaml(cfg)) # å†™å…¥æ•°æ®é…ç½®
        f.write("\n--- æ¨¡å‹é…ç½® (æ¥è‡ª ftt_model_config.json) ---\n")
        f.write(json.dumps(model_config, indent=2)) # [!] å†™å…¥æ¨¡å‹é…ç½®
        f.write("\n\n" + "="*30 + " æ‰€æœ‰å®éªŒæœ€ç»ˆæ€»ç»“ " + "="*30 + "\n")
        
        for final_result in final_results_summary:
            result_line = f"ç§å­: {final_result['seed']} | {final_result['result_line']}"
            print(result_line)
            f.write(result_line + "\n")

        params_header = f"\næœ€ä½³å‚æ•°çš„ä¾‹å­ (æ¥è‡ªæœ€åä¸€ä¸ªç§å­ {final_results_summary[-1]['seed']}):"
        params_details = str(final_results_summary[-1]['best_params'])
        
        print(params_header)
        print(params_details)
        f.write(params_header + "\n")
        f.write(params_details + "\n")
        f.write("="*80 + "\n")

    print(f"\nç»“æœå·²æˆåŠŸå†™å…¥åˆ°æ–‡ä»¶: {output_file_path}")


if __name__ == "__main__":
    main()