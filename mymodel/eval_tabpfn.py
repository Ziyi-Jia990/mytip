# eval_tabpfn.py
# -*- coding: utf-8 -*-
import argparse
import json
import numpy as np
import pandas as pd
from pathlib import Path
import os
import sys
import torch
import random

from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
# [ä¿®æ”¹] å¼•å…¥å›å½’æŒ‡æ ‡
from sklearn.metrics import (
    accuracy_score, f1_score, log_loss, roc_auc_score,
    mean_squared_error, mean_absolute_error, r2_score
)

# [ä¿®æ”¹] å¼•å…¥ Regressor
from tabpfn import TabPFNClassifier, TabPFNRegressor
from tabpfn_extensions.many_class.many_class_classifier import ManyClassClassifier

import hydra
from omegaconf import DictConfig, OmegaConf

# =========================
# å·¥å…·å‡½æ•°
# =========================

TEXT_LENGTH_DROP_THRESHOLD = 30
HIGH_CARDINALITY_THRESHOLD = 200
N_ENSEMBLE_CONFIGURATIONS = 16

def load_data(cfg: DictConfig):
    """
    (é‡æ„ç‰ˆ - åŸºäº field_lengths è‡ªåŠ¨åˆ¤æ–­åˆ—ç±»å‹)
    """
    target = cfg.target
    print(f"[INFO] æ­£åœ¨åŠ è½½ target: {target} (è‡ªåŠ¨æ¨æ–­åˆ—ç±»å‹)")

    try:
        # --- 1. åŠ è½½æ•°æ® ---
        X_train_full = pd.read_csv(cfg.data_train_eval_tabular, header=None)
        y_train_tensor = torch.load(cfg.labels_train_eval_tabular)
        
        # [ä¿®æ”¹] å›å½’ä»»åŠ¡ä¿æŒ floatï¼Œåˆ†ç±»ä»»åŠ¡è½¬ numpy
        if cfg.task == 'regression':
            y_train_full = y_train_tensor.float().numpy()
        else:
            y_train_full = y_train_tensor.numpy()

        X_test_full = pd.read_csv(cfg.data_test_eval_tabular, header=None)
        y_test_tensor = torch.load(cfg.labels_test_eval_tabular)
        
        if cfg.task == 'regression':
            y_test_full = y_test_tensor.float().numpy()
        else:
            y_test_full = y_test_tensor.numpy()

        # --- 2. åŠ è½½ field_lengths å¹¶è®¡ç®—ç´¢å¼• ---
        field_lengths_path = cfg.field_lengths_tabular
        print(f"[INFO] è¯»å–å­—æ®µé•¿åº¦æ–‡ä»¶: {field_lengths_path}")
        
        try:
            field_lengths = torch.load(field_lengths_path)
            if isinstance(field_lengths, torch.Tensor):
                field_lengths = field_lengths.cpu().numpy()
        except Exception:
            field_lengths = np.load(field_lengths_path)
        
        field_lengths = np.array(field_lengths).flatten()
        
        n_cols_data = X_train_full.shape[1]
        n_cols_lengths = len(field_lengths)
        if n_cols_data != n_cols_lengths:
            print(f"ğŸ”´ é”™è¯¯ï¼šCSV åˆ—æ•° ({n_cols_data}) ä¸ field_lengths é•¿åº¦ ({n_cols_lengths}) ä¸åŒ¹é…ï¼")
            sys.exit(1)

        con_indices = [i for i, fl in enumerate(field_lengths) if fl == 1]
        cat_indices = [i for i, fl in enumerate(field_lengths) if fl > 1]
        
        print(f"[INFO] è‡ªåŠ¨æ£€æµ‹ç»“æœ:")
        print(f"      - æ•°å€¼åˆ—æ•°é‡: {len(con_indices)}")
        print(f"      - ç±»åˆ«åˆ—æ•°é‡: {len(cat_indices)}")

        # --- 3. å®šä¹‰åˆ—å ---
        all_col_names = [f"col_{i}" for i in range(n_cols_data)]
        X_train_full.columns = all_col_names
        X_test_full.columns = all_col_names

        num_cols = [all_col_names[i] for i in con_indices]
        cat_cols = [all_col_names[i] for i in cat_indices]

        # --- 4. æ ‡ç­¾å¤„ç† (1-indexed -> 0-indexed) ---
        # [ä¿®æ”¹] åªæœ‰åˆ†ç±»ä»»åŠ¡æ‰æ‰§è¡Œæ­¤æ“ä½œ
        if cfg.task == 'classification':
            label_min = np.min(y_train_full)
            label_max = np.max(y_train_full)
            if label_min == 1 and label_max == cfg.num_classes:
                print(f"    [!] è­¦å‘Šï¼šæ£€æµ‹åˆ° 1-indexed æ ‡ç­¾ï¼Œæ­£åœ¨ä¿®å¤...")
                y_train_full = y_train_full - 1
                y_test_full = y_test_full - 1

        # --- 5. å¼ºåˆ¶ç±»å‹è½¬æ¢ ---
        if cat_cols:
            for col in cat_cols:
                X_train_full[col] = X_train_full[col].astype(str)
                X_test_full[col] = X_test_full[col].astype(str)

        if num_cols:
            for col in num_cols:
                X_train_full[col] = pd.to_numeric(X_train_full[col], errors='coerce').fillna(0)
                X_test_full[col] = pd.to_numeric(X_test_full[col], errors='coerce').fillna(0)

        return X_train_full, y_train_full, X_test_full, y_test_full, num_cols, cat_cols

    except Exception as e:
        print(f"ğŸ”´ åŠ è½½æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def build_preprocess(num_cols, cat_cols):
    transformers = []
    if num_cols:
        transformers.append(("num", StandardScaler(), num_cols))
    if cat_cols:
        transformers.append(("cat", OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols))
    
    return ColumnTransformer(transformers=transformers, remainder='drop', verbose_feature_names_out=False)

def get_subsample_indices(y, sample_size, seed, task):
    """
    [ä¿®æ”¹] é€šç”¨é‡‡æ ·å‡½æ•°ï¼š
    - åˆ†ç±»ä»»åŠ¡ï¼šåˆ†å±‚é‡‡æ ·
    - å›å½’ä»»åŠ¡ï¼šéšæœºé‡‡æ ·
    """
    sample_size = int(sample_size)
    if len(y) <= sample_size:
        return np.arange(len(y))
    
    # 1. å›å½’ä»»åŠ¡ç›´æ¥éšæœºé‡‡æ ·
    if task == 'regression':
        np.random.seed(seed)
        return np.random.choice(np.arange(len(y)), sample_size, replace=False)

    # 2. åˆ†ç±»ä»»åŠ¡é€»è¾‘
    unique_classes, counts = np.unique(y, return_counts=True)
    if len(unique_classes) < 2 or (counts < 2).any():
        np.random.seed(seed)
        return np.random.choice(np.arange(len(y)), sample_size, replace=False)

    sss = StratifiedShuffleSplit(n_splits=1, train_size=sample_size, random_state=seed)
    idx_all = np.arange(len(y))
    try:
        for sub_idx, _ in sss.split(idx_all, y):
            return sub_idx
    except ValueError:
        np.random.seed(seed)
        return np.random.choice(idx_all, sample_size, replace=False)

def evaluate_metrics(y_true, y_pred, task, y_proba=None):
    """
    [ä¿®æ”¹] æ”¯æŒå›å½’å’Œåˆ†ç±»æŒ‡æ ‡
    """
    res = {}
    
    if task == 'classification':
        res["accuracy"] = accuracy_score(y_true, y_pred)
        res["macro_f1"] = f1_score(y_true, y_pred, average='macro')
        res["weighted_f1"] = f1_score(y_true, y_pred, average='weighted')
        
        if y_proba is not None:
            try:
                if y_proba.shape[1] == 2:
                    res["auc"] = roc_auc_score(y_true, y_proba[:, 1])
                else:
                    res["auc_macro_ovr"] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')
            except:
                pass
                
    elif task == 'regression':
        # [æ–°å¢] å›å½’æŒ‡æ ‡
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        res["rmse"] = rmse
        res["mae"] = mae
        res["r2"] = r2
        
    return res

# =========================
# ä¸»æµç¨‹
# =========================

@hydra.main(version_base=None, config_path="../configs", config_name="config")
def main(cfg: DictConfig):
    seeds = [2022, 2023, 2024]
    results_all = []

    # ç¡®ä¿ cfg ä¸­æœ‰ task å­—æ®µ
    if 'task' not in cfg:
        print("âš ï¸ Config ä¸­ç¼ºå°‘ 'task' å­—æ®µï¼Œé»˜è®¤è®¾ä¸º 'classification'")
        cfg.task = 'classification'

    for seed in seeds:
        print(f"\nğŸš€ æ­£åœ¨è¿è¡Œ seed = {seed} | Task: {cfg.task}")
        cfg.seed = seed

        # --- è·¯å¾„è§£æ ---
        data_root = cfg.get('data_base')
        if data_root:
            path_keys = [
                'labels_train_eval_tabular', 'labels_test_eval_tabular',
                'data_train_eval_tabular', 'data_test_eval_tabular',
                'field_lengths_tabular'
            ]
            for key in path_keys:
                if key in cfg and cfg[key] and not os.path.isabs(cfg[key]):
                    cfg[key] = os.path.join(data_root, cfg[key])

        TRAIN_SAMPLE_THRESHOLD = cfg.get('train_sample_max', 8000)
        TEST_SAMPLE_THRESHOLD = cfg.get('test_sample_max', 2000)

        # 1. åŠ è½½æ•°æ®
        X_train_full, y_train_full, X_test_full, y_test_full, num_cols, cat_cols = load_data(cfg)

        # 2. é¢„å¤„ç†
        preprocess = build_preprocess(num_cols, cat_cols)

        # 3. è®­ç»ƒé›†é‡‡æ · (è‡ªåŠ¨æ ¹æ® task é€‰æ‹©é‡‡æ ·æ–¹å¼)
        if len(y_train_full) > TRAIN_SAMPLE_THRESHOLD:
            sample_size = TRAIN_SAMPLE_THRESHOLD
        else:
            sample_size = len(y_train_full)

        sub_idx = get_subsample_indices(y_train_full, sample_size, seed, cfg.task)
        X_train_sampled = X_train_full.iloc[sub_idx]
        y_train_sampled = y_train_full[sub_idx]

        # 4. ç‰¹å¾è½¬æ¢
        print("æ­£åœ¨è¿›è¡Œç‰¹å¾é¢„å¤„ç†...")
        X_train_np = preprocess.fit_transform(X_train_sampled)
        X_test_np  = preprocess.transform(X_test_full)
        print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: è®­ç»ƒé›† {X_train_np.shape}, æµ‹è¯•é›† {X_test_np.shape}")

        # 5. æ¨¡å‹åˆå§‹åŒ– [ä¿®æ”¹ï¼šæ ¹æ®ä»»åŠ¡é€‰æ‹©æ¨¡å‹]
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        if cfg.task == 'classification':
            if cfg.num_classes > 10:
                base_clf = TabPFNClassifier(n_estimators=N_ENSEMBLE_CONFIGURATIONS, device=device)
                clf = ManyClassClassifier(estimator=base_clf, alphabet_size=10, random_state=seed)
            else:
                clf = TabPFNClassifier(n_estimators=N_ENSEMBLE_CONFIGURATIONS, device=device)
        elif cfg.task == 'regression':
            # [æ–°å¢] å›å½’æ¨¡å‹
            print("æ­£åœ¨åˆå§‹åŒ– TabPFNRegressor...")
            clf = TabPFNRegressor(n_estimators=N_ENSEMBLE_CONFIGURATIONS, device=device)
        else:
            raise ValueError(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {cfg.task}")

        # 6. è®­ç»ƒ
        clf.fit(X_train_np, y_train_sampled)

        # 7. æµ‹è¯•é›†é‡‡æ ·ä¸è¯„ä¼°
        X_test_eval = X_test_np
        y_test_eval = y_test_full

        if len(X_test_np) > TEST_SAMPLE_THRESHOLD:
            # å›å½’ç”¨æ™®é€š splitï¼Œåˆ†ç±»ç”¨ stratify
            if cfg.task == 'classification':
                stratify_target = y_test_full
            else:
                stratify_target = None # å›å½’ä¸èƒ½ stratify

            X_test_eval, _, y_test_eval, _ = train_test_split(
                X_test_np, y_test_full,
                train_size=TEST_SAMPLE_THRESHOLD,
                stratify=stratify_target,
                random_state=seed
            )
        
        # 8. é¢„æµ‹ [ä¿®æ”¹ï¼šåŒºåˆ†åˆ†ç±»å’Œå›å½’]
        test_proba = None
        test_pred = None

        if cfg.task == 'classification':
            test_proba = clf.predict_proba(X_test_eval)
            test_pred  = np.argmax(test_proba, axis=1)
        else:
            # å›å½’æ²¡æœ‰ predict_proba
            test_pred = clf.predict(X_test_eval)
        
        # 9. è®¡ç®—æŒ‡æ ‡
        metrics = evaluate_metrics(y_test_eval, test_pred, cfg.task, test_proba)
        print(f"[RESULT] seed={seed} æŒ‡æ ‡: {json.dumps(metrics, indent=2)}")
        
        results_all.append({"seed": seed, "results": metrics})

    # ä¿å­˜ç»“æœ
    output_file = "result/tabpfn_results.json" # å»ºè®®ç”¨ç›¸å¯¹è·¯å¾„æˆ–ä» cfg è¯»å–
    if os.path.exists("/home/debian/TIP/mymodel/result/"): # å¦‚æœåŸå§‹ç»å¯¹è·¯å¾„å­˜åœ¨
        output_file = "/home/debian/TIP/mymodel/result/tabpfn_results.json"
        
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, 'w') as f:
        json.dump(results_all, f, indent=2)
    print(f"ç»“æœå·²ä¿å­˜: {output_file}")

if __name__ == "__main__":
    main() 