# eval_tabpfn.py
# -*- coding: utf-8 -*-
import argparse
import json
import numpy as np
import pandas as pd
from pathlib import Path
import os
import sys
import torch # æ–°å¢

from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
# <--- ä¿®æ”¹å¯¼å…¥ï¼šå¢åŠ äº† roc_auc_score ---
from sklearn.metrics import accuracy_score, f1_score, log_loss, roc_auc_score

from tabpfn import TabPFNClassifier
from tabpfn_extensions.many_class.many_class_classifier import ManyClassClassifier

# --- Hydra å¯¼å…¥ ---
import hydra
from omegaconf import DictConfig, OmegaConf

# =========================
# å·¥å…·å‡½æ•°ï¼ˆä¿®æ”¹ä¸ºä» cfg è¯»å–ï¼‰
# =========================

# ä» cfg ä¸­è¯»å–å›ºå®šçš„é˜ˆå€¼ï¼Œå¦‚æœcfgä¸­æ²¡æœ‰ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼
TEXT_LENGTH_DROP_THRESHOLD = 30
HIGH_CARDINALITY_THRESHOLD = 200
N_ENSEMBLE_CONFIGURATIONS = 16


def load_data(cfg: DictConfig):
    """
    (é‡æ„ç‰ˆ) 
    é€šç”¨æ•°æ®åŠ è½½å™¨ï¼Œç”¨äºåŠ è½½å·²é¢„å¤„ç†çš„ .csv å’Œ .pt æ–‡ä»¶ã€‚
    """
    target = cfg.target
    print(f"[INFO] æ­£åœ¨åŠ è½½ target: {target} (é€šç”¨åŠ è½½å™¨)")

    try:
        # --- 1. åŠ è½½æ•°æ® ---
        # è®­ç»ƒé›† = train_eval_tabular
        # æµ‹è¯•é›† = val_eval_tabular (ä¸ LGBM è„šæœ¬ä¿æŒä¸€è‡´ï¼Œä½¿ç”¨éªŒè¯é›†è¿›è¡Œè¯„ä¼°)
        
        # åŠ è½½è®­ç»ƒé›†
        X_train_full = pd.read_csv(cfg.data_train_eval_tabular, header=None)
        # [!] ç§»é™¤ 'weights_only' ä»¥å…¼å®¹æ—§ç‰ˆ PyTorch
        y_train_tensor = torch.load(cfg.labels_train_eval_tabular) 
        y_train_full = y_train_tensor.numpy()

        # åŠ è½½æµ‹è¯•é›† (æˆ‘ä»¬ä½¿ç”¨ 'val' æ•°æ®é›†)
        X_test_full = pd.read_csv(cfg.data_test_eval_tabular, header=None)
        y_test_tensor = torch.load(cfg.labels_test_eval_tabular)
        y_test_full = y_test_tensor.numpy()

        # --- 2. æ£€æŸ¥å¹¶ä¿®å¤ 1-indexed æ ‡ç­¾ ---
        if cfg.task == 'classification':
            label_min = np.min(y_train_full)
            label_max = np.max(y_train_full)
            
            if label_min == 1 and label_max == cfg.num_classes:
                print(f"    [!] è­¦å‘Šï¼šæ£€æµ‹åˆ° 1-indexed æ ‡ç­¾ (min={label_min}, max={label_max})ã€‚")
                print("        æ­£åœ¨å‡å» 1 ä½¿å…¶å˜ä¸º 0-indexedã€‚")
                y_train_full = y_train_full - 1
                y_test_full = y_test_full - 1
            elif label_min < 0 or label_max >= cfg.num_classes:
                print(f"ğŸ”´ é”™è¯¯ï¼šæ ‡ç­¾è¶Šç•Œï¼")
                print(f"       æ¨¡å‹æœ‰ {cfg.num_classes} ä¸ªç±»åˆ« (é¢„æœŸ 0 åˆ° {cfg.num_classes - 1})")
                print(f"       ä½†æ ‡ç­¾ä¸­å‘ç° æœ€å°å€¼={label_min}, æœ€å¤§å€¼={label_max}")
                sys.exit(1)
        
        # --- 3. å®šä¹‰åˆ—åå’Œç±»å‹ ---
        num_con = cfg.num_con
        num_cat = cfg.num_cat
        
        if X_train_full.shape[1] != (num_con + num_cat):
            print(f"ğŸ”´ é”™è¯¯ï¼šåŠ è½½çš„ X_train æœ‰ {X_train_full.shape[1]} åˆ—, ä½† config é¢„æœŸ {num_con + num_cat} åˆ—ã€‚")
            sys.exit(1)

        # åˆ›å»ºåˆ—å
        num_cols = [f"num_{i}" for i in range(num_con)]
        cat_cols = [f"cat_{i}" for i in range(num_cat)]
        all_cols = num_cols + cat_cols

        X_train_full.columns = all_cols
        X_test_full.columns = all_cols
        
        # --- 4. å…³é”®ï¼šå¼ºåˆ¶è½¬æ¢ç±»å‹ ---
        # æˆ‘ä»¬å¿…é¡»å¼ºåˆ¶ cat_cols ä¸º 'object'/'str'ï¼Œ
        # è¿™æ · build_preprocess ä¸­çš„ `is_numeric_dtype` æ‰èƒ½æ­£ç¡®å·¥ä½œã€‚
        for col in cat_cols:
            X_train_full[col] = X_train_full[col].astype(str)
            X_test_full[col] = X_test_full[col].astype(str)

        # åˆ—å¯¹é½ (åœ¨ build_preprocess ä¹‹å‰æ˜¯å¤šä½™çš„ï¼Œä½†ä¿ç•™ä»¥é˜²ä¸‡ä¸€)
        X_test_full = X_test_full[X_train_full.columns]

        print(f"[INFO] æ•°å€¼åˆ— (åŸºäºconfig): {num_cols}")
        print(f"[INFO] åˆ†ç±»åˆ— (åŸºäºconfig): {cat_cols}")

        return X_train_full, y_train_full, X_test_full, y_test_full, num_cols, cat_cols

    except FileNotFoundError as e:
        print(f"ğŸ”´ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {e.filename}ã€‚")
        print("    è¯·ç¡®ä¿ config ä¸­çš„è·¯å¾„æ­£ç¡®ï¼Œå¹¶ä¸”é¢„å¤„ç†è„šæœ¬å·²æˆåŠŸè¿è¡Œã€‚")
        sys.exit(1)
    except KeyError as e:
        print(f"ğŸ”´ é”™è¯¯ï¼šConfig æ–‡ä»¶ä¸­ç¼ºå°‘å…³é”®çš„é”®: {e}")
        print("    è¯·ç¡®ä¿ cfg åŒ…å« 'data_train_eval_tabular', 'labels_train_eval_tabular', 'data_val_eval_tabular', 'labels_val_eval_tabular', 'num_con', 'num_cat', 'task', 'num_classes'")
        sys.exit(1)
    except Exception as e:
        print(f"ğŸ”´ åŠ è½½æ•°æ®æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def build_preprocess(num_cols, cat_cols):
    """
    ä¿®æ”¹ï¼šç¡®ä¿è¾“å‡ºä¸º TabPFN éœ€è¦çš„å¯†é›† (dense) çŸ©é˜µã€‚
    """
    transformers = []
    if num_cols:
        transformers.append(("num", StandardScaler(), num_cols))
    if cat_cols:
        # å…³é”®ä¿®å¤ï¼šsparse=True -> sparse_output=False
        transformers.append(("cat", OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)) 
    
    # å…³é”®ä¿®å¤ï¼šsparse_threshold=1.0 -> 0.0
    return ColumnTransformer(transformers=transformers, remainder='drop', sparse_threshold=0.0)

def stratified_subsample_indices(y, sample_size, seed):
    """
    ä» y ä¸­è·å–ç”¨äºé‡‡æ ·çš„ç´¢å¼• (æ‚¨çš„ä»£ç å·²æ­£ç¡®)
    """
    # ç¡®ä¿ sample_size æ˜¯æ•´æ•°
    sample_size = int(sample_size)
    if len(y) <= sample_size:
        return np.arange(len(y))
    
    # ç¡®ä¿ y ä¸­è‡³å°‘æœ‰2ä¸ªç±»åˆ«ï¼Œæˆ–è€…è¶³å¤Ÿçš„æ ·æœ¬
    unique_classes, counts = np.unique(y, return_counts=True)
    if len(unique_classes) < 2 or (counts < 2).any():
        print("[WARNING] ç±»åˆ«å¤ªå°‘æˆ–æ ·æœ¬ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œåˆ†å±‚é‡‡æ ·ï¼Œé€€å›åˆ°éšæœºé‡‡æ ·ã€‚")
        np.random.seed(seed)
        return np.random.choice(np.arange(len(y)), sample_size, replace=False)

    sss = StratifiedShuffleSplit(n_splits=1, train_size=sample_size, random_state=seed)
    idx_all = np.arange(len(y))
    try:
        for sub_idx, _ in sss.split(idx_all, y):
            return sub_idx
    except ValueError as e:
        print(f"[WARNING] åˆ†å±‚é‡‡æ ·å¤±è´¥ ({e})ï¼Œé€€å›åˆ°éšæœºé‡‡æ ·ã€‚")
        np.random.seed(seed)
        return np.random.choice(idx_all, sample_size, replace=False)

# <--- å‡½æ•°å·²ä¿®æ”¹ (å¢åŠ AUC) ---
def evaluate_metrics(y_true, y_pred, y_proba=None):
    """
    (å·²ä¿®æ”¹ï¼šå¢åŠ äº† AUC è®¡ç®—)
    """
    res = {
        "accuracy": accuracy_score(y_true, y_pred),
        "macro_f1": f1_score(y_true, y_pred, average='macro'),
        "weighted_f1": f1_score(y_true, y_pred, average='weighted'),
    }
    
    if y_proba is not None:
        # è·å–æ¦‚ç‡çŸ©é˜µä¸­çš„ç±»åˆ«æ•° å’Œ çœŸå®æ ‡ç­¾ä¸­çš„ç±»åˆ«æ•°
        n_classes_proba = y_proba.shape[1]
        unique_true_classes = np.unique(y_true)
        n_classes_true = len(unique_true_classes)

        # --- 1. LogLoss (åŸæœ‰é€»è¾‘) ---
        try:
            # ç¡®ä¿ y_proba çš„åˆ—æ•°ä¸ç±»åˆ«æ•°ä¸€è‡´
            # ç¡®ä¿ y_true ä¸­çš„æ ‡ç­¾åœ¨ [0, n_classes-1] èŒƒå›´å†…
            if y_true.max() >= n_classes_proba:
                print(f"[WARNING] y_true åŒ…å«æ ‡ç­¾ {y_true.max()}ï¼Œä½† y_proba åªæœ‰ {n_classes_proba} åˆ—ã€‚LogLoss å¯èƒ½ä¸å‡†ç¡®ã€‚")
            
            res["log_loss"] = log_loss(y_true, y_proba, labels=np.arange(n_classes_proba))
        except Exception as e:
            print(f"[WARNING] æ— æ³•è®¡ç®— LogLoss: {e}")
            pass

        # --- 2. AUC (æ–°å¢é€»è¾‘) ---
        try:
            # æ£€æŸ¥ y_true ä¸­æ˜¯å¦åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œè¿™ä¼šå¯¼è‡´ AUC æ— æ³•è®¡ç®—
            if n_classes_true < 2:
                print(f"[WARNING] y_true ä¸­åªæœ‰ä¸€ä¸ªç±»åˆ« ({unique_true_classes})ï¼Œè·³è¿‡ AUC è®¡ç®—ã€‚")
            
            # æƒ…å†µ A: äºŒåˆ†ç±» (y_proba æœ‰ 2 åˆ—)
            elif n_classes_proba == 2:
                # roc_auc_score éœ€è¦ y_true å’Œ *æ­£ç±»*çš„æ¦‚ç‡
                res["auc"] = roc_auc_score(y_true, y_proba[:, 1])
            
            # æƒ…å†µ B: å¤šåˆ†ç±» (y_proba æœ‰ >2 åˆ—)
            else:
                res["auc_macro_ovr"] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')
                res["auc_weighted_ovr"] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='weighted')
                # æ‚¨ä¹Ÿå¯ä»¥é€‰æ‹© 'ovo'
                # res["auc_macro_ovo"] = roc_auc_score(y_true, y_proba, multi_class='ovo', average='macro')

        except Exception as e:
            print(f"[WARNING] æ— æ³•è®¡ç®— AUC: {e}")
            pass

    return res
# <--- å‡½æ•°ä¿®æ”¹ç»“æŸ ---

# =========================
# ä¸»æµç¨‹ (Hydra)
# =========================

@hydra.main(version_base=None, config_path="../configs", config_name="config")
def main(cfg: DictConfig):

    seeds = [2022, 2023, 2024]
    results_all = []

    for seed in seeds:
        print(f"\n==============================")
        print(f"ğŸš€ æ­£åœ¨è¿è¡Œ seed = {seed}")
        print(f"==============================")

        cfg.seed = seed   # åŠ¨æ€ä¿®æ”¹é…ç½®ä¸­çš„ seed

        # -------------------------------------------------
        # ä¸‹é¢æ˜¯ä½ åŸæ¥ main(cfg) å†…éƒ¨çš„æ•´ä¸ªæµç¨‹â€”â€”ä¿æŒä¸å˜
        # -------------------------------------------------

        print("--- 1.A. æ­£åœ¨è§£ææ•°æ®è·¯å¾„ ---")
        data_root = cfg.get('data_base') 
        if data_root is not None:
            print(f"    æ£€æµ‹åˆ° 'data_root'ï¼Œå°†ä¸ºæ‰€æœ‰æ•°æ®æ–‡ä»¶æ·»åŠ å‰ç¼€: {data_root}")
            path_keys = [
                'labels_train', 'labels_val',
                'data_train_imaging', 'data_val_imaging',
                'data_train_tabular', 'data_val_tabular',
                'field_lengths_tabular',
                'data_train_eval_tabular', 'labels_train_eval_tabular',
                'data_val_eval_tabular', 'labels_val_eval_tabular',
                'data_test_eval_tabular', 'labels_test_eval_tabular',
                'data_train_eval_imaging', 'labels_train_eval_imaging',
                'data_val_eval_imaging', 'labels_val_eval_imaging',
                'data_test_eval_imaging', 'labels_test_eval_imaging'
            ]
            for key in path_keys:
                if key in cfg and cfg[key] is not None:
                    cfg[key] = os.path.join(data_root, cfg[key])
        else:
            print("    æœªæä¾› 'data_root'ã€‚å°†å‡å®š config ä¸­çš„è·¯å¾„å·²ç»æ˜¯æ­£ç¡®çš„ã€‚")

        print("\n--- æœ€ç»ˆé…ç½® (è·¯å¾„å·²è§£æ): ---")
        print(OmegaConf.to_yaml(cfg))
        print("--------------------")
        print(f"Hydra å·¥ä½œç›®å½•: {os.getcwd()}")
        print("--------------------")

        TRAIN_SAMPLE_THRESHOLD = cfg.get('train_sample_max', 8000)
        TEST_SAMPLE_THRESHOLD = cfg.get('test_sample_max', 2000)

        # è¯»å–æ•°æ®
        X_train_full, y_train_full, X_test_full, y_test_full, num_cols, cat_cols = load_data(cfg)

        # é¢„å¤„ç†
        preprocess = build_preprocess(num_cols, cat_cols)

        # é‡‡æ ·
        if len(y_train_full) > TRAIN_SAMPLE_THRESHOLD:
            sample_size = TRAIN_SAMPLE_THRESHOLD
        else:
            sample_size = len(y_train_full)

        sub_idx = stratified_subsample_indices(y_train_full, sample_size, seed)
        X_train_sampled = X_train_full.iloc[sub_idx]
        y_train_sampled = y_train_full[sub_idx]

        X_train_np = preprocess.fit_transform(X_train_sampled)
        X_test_np  = preprocess.transform(X_test_full)

        n_ensemble = N_ENSEMBLE_CONFIGURATIONS
        device = 'cuda' if torch.cuda.is_available() else 'cpu'

        print(f"[INFO] ä½¿ç”¨è®¾å¤‡: {device}")

        # ---- å…³é”®ï¼šTabPFN ä¸è¦ batch_sizeï¼Œé¿å…æŠ¥é”™ ----
        if cfg.num_classes > 10:
            print(f"[INFO] å¤šåˆ†ç±»ï¼Œä½¿ç”¨ ManyClassClassifier")
            base_clf = TabPFNClassifier(
                n_estimators=n_ensemble,
                device=device
            )
            clf = ManyClassClassifier(
                estimator=base_clf,
                alphabet_size=10,
                random_state=seed,
                verbose=1,
            )
        else:
            clf = TabPFNClassifier(
                n_estimators=n_ensemble,
                device=device
            )

        print("Fitting TabPFN...")
        clf.fit(X_train_np, y_train_sampled)
        print("Fit complete.")

        # æµ‹è¯•é›†é‡‡æ ·
        X_test_sampled = X_test_np
        y_test_sampled = y_test_full

        if len(X_test_np) > TEST_SAMPLE_THRESHOLD:
            try:
                X_test_sampled, _, y_test_sampled, _ = train_test_split(
                    X_test_np, y_test_full,
                    train_size=TEST_SAMPLE_THRESHOLD,
                    stratify=y_test_full,
                    random_state=seed
                )
            except Exception as e:
                print("æµ‹è¯•é›†é‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨å®Œæ•´æµ‹è¯•é›†ã€‚")

        # é¢„æµ‹
        test_proba = clf.predict_proba(X_test_sampled)
        test_pred  = np.argmax(test_proba, axis=1)

        metrics = evaluate_metrics(y_test_sampled, test_pred, test_proba)

        print(f"[RESULT] seed={seed} æµ‹è¯•é›†æŒ‡æ ‡ï¼š")
        print(json.dumps(metrics, indent=2))

        # ä¿å­˜å½“å‰ seed çš„ç»“æœåˆ°åˆ—è¡¨
        results_all.append({
            "seed": seed,
            "results": metrics
        })

    # ---------------------------------------
    # ç»Ÿä¸€å†™å…¥ç»“æœæ–‡ä»¶ï¼ˆä¸€æ¬¡æ€§å†™åˆæ³• JSONï¼‰
    # ---------------------------------------
    output_file = "/home/debian/TIP/mymodel/result/tabpfn_results.json"
    with open(output_file, 'w') as f:
        json.dump(results_all, f, indent=2)

    print(f"\nğŸ‰ æ‰€æœ‰ seed è¿è¡Œå®Œæˆï¼Œç»“æœå·²å†™å…¥ï¼š{output_file}\n")



if __name__ == "__main__":
    main()
