# eval_tabpfn.py
# -*- coding: utf-8 -*-
import argparse
import json
import numpy as np
import pandas as pd
from pathlib import Path
import os
import sys
import torch

from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, f1_score, log_loss, roc_auc_score

from tabpfn import TabPFNClassifier
from tabpfn_extensions.many_class.many_class_classifier import ManyClassClassifier

import hydra
from omegaconf import DictConfig, OmegaConf

# =========================
# å·¥å…·å‡½æ•°
# =========================

TEXT_LENGTH_DROP_THRESHOLD = 30
HIGH_CARDINALITY_THRESHOLD = 200
N_ENSEMBLE_CONFIGURATIONS = 16

def load_data(cfg: DictConfig):
    """
    (é‡æ„ç‰ˆ - åŸºäº field_lengths è‡ªåŠ¨åˆ¤æ–­åˆ—ç±»å‹)
    """
    target = cfg.target
    print(f"[INFO] æ­£åœ¨åŠ è½½ target: {target} (è‡ªåŠ¨æ¨æ–­åˆ—ç±»å‹)")

    try:
        # --- 1. åŠ è½½æ•°æ® ---
        X_train_full = pd.read_csv(cfg.data_train_eval_tabular, header=None)
        y_train_tensor = torch.load(cfg.labels_train_eval_tabular)
        y_train_full = y_train_tensor.numpy()

        X_test_full = pd.read_csv(cfg.data_test_eval_tabular, header=None)
        y_test_tensor = torch.load(cfg.labels_test_eval_tabular)
        y_test_full = y_test_tensor.numpy()

        # --- 2. åŠ è½½ field_lengths å¹¶è®¡ç®—ç´¢å¼• ---
        # å‡è®¾ field_lengths_tabular æ˜¯ä¸€ä¸ª .pt æ–‡ä»¶ (Torch Tensor) æˆ– .npy
        # å¦‚æœæ˜¯å…¶ä»–æ ¼å¼ (å¦‚json)ï¼Œè¯·æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
        field_lengths_path = cfg.field_lengths_tabular
        print(f"[INFO] è¯»å–å­—æ®µé•¿åº¦æ–‡ä»¶: {field_lengths_path}")
        
        try:
            # å°è¯•ä½œä¸º torch tensor åŠ è½½
            field_lengths = torch.load(field_lengths_path)
            if isinstance(field_lengths, torch.Tensor):
                field_lengths = field_lengths.numpy()
        except Exception:
            # å›é€€ï¼šå°è¯•ä½œä¸º numpy åŠ è½½
            field_lengths = np.load(field_lengths_path)
        
        # å±•å¹³ä»¥é˜²ä¸‡ä¸€
        field_lengths = np.array(field_lengths).flatten()
        
        # æ ¡éªŒåˆ—æ•°æ˜¯å¦åŒ¹é…
        n_cols_data = X_train_full.shape[1]
        n_cols_lengths = len(field_lengths)
        if n_cols_data != n_cols_lengths:
            print(f"ğŸ”´ é”™è¯¯ï¼šCSV åˆ—æ•° ({n_cols_data}) ä¸ field_lengths é•¿åº¦ ({n_cols_lengths}) ä¸åŒ¹é…ï¼")
            sys.exit(1)

        # === æ ¸å¿ƒä¿®æ”¹é€»è¾‘ ===
        # TIP å‡è®¾ï¼šfield_len == 1 -> è¿ç»­ç‰¹å¾ï¼› >1 -> ç±»åˆ«ç‰¹å¾
        con_indices = [i for i, fl in enumerate(field_lengths) if fl == 1]
        cat_indices = [i for i, fl in enumerate(field_lengths) if fl > 1]
        
        print(f"[INFO] è‡ªåŠ¨æ£€æµ‹ç»“æœ:")
        print(f"      - æ•°å€¼åˆ—æ•°é‡: {len(con_indices)}")
        print(f"      - ç±»åˆ«åˆ—æ•°é‡: {len(cat_indices)}")

        # --- 3. å®šä¹‰åˆ—å ---
        # ç»™æ‰€æœ‰åˆ—ä¸€ä¸ªé€šç”¨åå­—ï¼Œæ–¹ä¾¿åç»­æŒ‰åå­—ç´¢å¼•
        all_col_names = [f"col_{i}" for i in range(n_cols_data)]
        X_train_full.columns = all_col_names
        X_test_full.columns = all_col_names

        # æ ¹æ®ç´¢å¼•æå–å¯¹åº”çš„åˆ—ååˆ—è¡¨
        num_cols = [all_col_names[i] for i in con_indices]
        cat_cols = [all_col_names[i] for i in cat_indices]

        # --- 4. æ ‡ç­¾å¤„ç† (1-indexed -> 0-indexed) ---
        if cfg.task == 'classification':
            label_min = np.min(y_train_full)
            label_max = np.max(y_train_full)
            if label_min == 1 and label_max == cfg.num_classes:
                print(f"    [!] è­¦å‘Šï¼šæ£€æµ‹åˆ° 1-indexed æ ‡ç­¾ï¼Œæ­£åœ¨ä¿®å¤...")
                y_train_full = y_train_full - 1
                y_test_full = y_test_full - 1

        # --- 5. å¼ºåˆ¶ç±»å‹è½¬æ¢ ---
        # TabPFN é¢„å¤„ç†éœ€è¦ç±»åˆ«åˆ—ä¸ºå­—ç¬¦ä¸²
        if cat_cols:
            for col in cat_cols:
                X_train_full[col] = X_train_full[col].astype(str)
                X_test_full[col] = X_test_full[col].astype(str)

        # ç¡®ä¿æ•°å€¼åˆ—æ˜¯ float
        if num_cols:
            for col in num_cols:
                X_train_full[col] = pd.to_numeric(X_train_full[col], errors='coerce').fillna(0)
                X_test_full[col] = pd.to_numeric(X_test_full[col], errors='coerce').fillna(0)

        return X_train_full, y_train_full, X_test_full, y_test_full, num_cols, cat_cols

    except Exception as e:
        print(f"ğŸ”´ åŠ è½½æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def build_preprocess(num_cols, cat_cols):
    """
    æ„å»ºé¢„å¤„ç†å™¨
    """
    transformers = []
    if num_cols:
        transformers.append(("num", StandardScaler(), num_cols))
    if cat_cols:
        # sparse_output=False å¯¹ TabPFN è‡³å…³é‡è¦
        transformers.append(("cat", OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols))
    
    return ColumnTransformer(transformers=transformers, remainder='drop', verbose_feature_names_out=False)

def stratified_subsample_indices(y, sample_size, seed):
    """(ä¿æŒä¸å˜)"""
    sample_size = int(sample_size)
    if len(y) <= sample_size:
        return np.arange(len(y))
    
    unique_classes, counts = np.unique(y, return_counts=True)
    if len(unique_classes) < 2 or (counts < 2).any():
        np.random.seed(seed)
        return np.random.choice(np.arange(len(y)), sample_size, replace=False)

    sss = StratifiedShuffleSplit(n_splits=1, train_size=sample_size, random_state=seed)
    idx_all = np.arange(len(y))
    try:
        for sub_idx, _ in sss.split(idx_all, y):
            return sub_idx
    except ValueError:
        np.random.seed(seed)
        return np.random.choice(idx_all, sample_size, replace=False)

def evaluate_metrics(y_true, y_pred, y_proba=None):
    """(ä¿æŒä¸å˜)"""
    res = {
        "accuracy": accuracy_score(y_true, y_pred),
        "macro_f1": f1_score(y_true, y_pred, average='macro'),
        "weighted_f1": f1_score(y_true, y_pred, average='weighted'),
    }
    if y_proba is not None:
        try:
            if y_proba.shape[1] == 2:
                res["auc"] = roc_auc_score(y_true, y_proba[:, 1])
            else:
                res["auc_macro_ovr"] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')
        except:
            pass
    return res

# =========================
# ä¸»æµç¨‹
# =========================

@hydra.main(version_base=None, config_path="../configs", config_name="config")
def main(cfg: DictConfig):
    seeds = [2022, 2023, 2024]
    results_all = []

    for seed in seeds:
        print(f"\nğŸš€ æ­£åœ¨è¿è¡Œ seed = {seed}")
        cfg.seed = seed

        # --- è·¯å¾„è§£æ (å¯é€‰ï¼Œä¿ç•™åŸæ¥çš„é€»è¾‘) ---
        data_root = cfg.get('data_base')
        if data_root:
            path_keys = [
                'labels_train_eval_tabular', 'labels_test_eval_tabular',
                'data_train_eval_tabular', 'data_test_eval_tabular',
                'field_lengths_tabular' # ç¡®ä¿è¿™ä¸ªä¹Ÿåœ¨æ›´æ–°åˆ—è¡¨é‡Œ
            ]
            for key in path_keys:
                if key in cfg and cfg[key] and not os.path.isabs(cfg[key]):
                    cfg[key] = os.path.join(data_root, cfg[key])

        TRAIN_SAMPLE_THRESHOLD = cfg.get('train_sample_max', 8000)
        TEST_SAMPLE_THRESHOLD = cfg.get('test_sample_max', 2000)

        # 1. åŠ è½½æ•°æ® (ä½¿ç”¨ä¿®æ”¹åçš„å‡½æ•°)
        X_train_full, y_train_full, X_test_full, y_test_full, num_cols, cat_cols = load_data(cfg)

        # 2. é¢„å¤„ç†
        preprocess = build_preprocess(num_cols, cat_cols)

        # 3. è®­ç»ƒé›†é‡‡æ ·
        if len(y_train_full) > TRAIN_SAMPLE_THRESHOLD:
            sample_size = TRAIN_SAMPLE_THRESHOLD
        else:
            sample_size = len(y_train_full)

        sub_idx = stratified_subsample_indices(y_train_full, sample_size, seed)
        X_train_sampled = X_train_full.iloc[sub_idx]
        y_train_sampled = y_train_full[sub_idx]

        # 4. ç‰¹å¾è½¬æ¢
        print("æ­£åœ¨è¿›è¡Œç‰¹å¾é¢„å¤„ç†...")
        X_train_np = preprocess.fit_transform(X_train_sampled)
        X_test_np  = preprocess.transform(X_test_full)
        print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: è®­ç»ƒé›† {X_train_np.shape}, æµ‹è¯•é›† {X_test_np.shape}")

        # 5. æ¨¡å‹åˆå§‹åŒ–
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        if cfg.num_classes > 10:
            base_clf = TabPFNClassifier(n_estimators=N_ENSEMBLE_CONFIGURATIONS, device=device)
            clf = ManyClassClassifier(estimator=base_clf, alphabet_size=10, random_state=seed)
        else:
            clf = TabPFNClassifier(n_estimators=N_ENSEMBLE_CONFIGURATIONS, device=device)

        # 6. è®­ç»ƒ
        clf.fit(X_train_np, y_train_sampled)

        # 7. æµ‹è¯•é›†é‡‡æ ·ä¸è¯„ä¼°
        X_test_eval = X_test_np
        y_test_eval = y_test_full

        if len(X_test_np) > TEST_SAMPLE_THRESHOLD:
            X_test_eval, _, y_test_eval, _ = train_test_split(
                X_test_np, y_test_full,
                train_size=TEST_SAMPLE_THRESHOLD,
                stratify=y_test_full,
                random_state=seed
            )
        
        test_proba = clf.predict_proba(X_test_eval)
        test_pred  = np.argmax(test_proba, axis=1)
        
        metrics = evaluate_metrics(y_test_eval, test_pred, test_proba)
        print(f"[RESULT] seed={seed} æŒ‡æ ‡: {json.dumps(metrics, indent=2)}")
        
        results_all.append({"seed": seed, "results": metrics})

    # ä¿å­˜ç»“æœ
    output_file = "/home/debian/TIP/mymodel/result/tabpfn_results.json"
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, 'w') as f:
        json.dump(results_all, f, indent=2)
    print(f"ç»“æœå·²ä¿å­˜: {output_file}")

if __name__ == "__main__":
    main()