import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold
# --- ä¿®æ”¹ç‚¹ 1: å¼•å…¥ mean_absolute_error å’Œ r2_score ---
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_squared_error, mean_absolute_error, r2_score
import sys
import torch
import os

# --- Hydra å¯¼å…¥ ---
import hydra
from omegaconf import DictConfig, OmegaConf

def load_and_preprocess_data(cfg: DictConfig):
    task = cfg.get('task', 'classification')
    print(f"--- 1. æ­£åœ¨ä¸º Target: '{cfg.target}' (ä»»åŠ¡: {task}) (é€šç”¨LGBMåŠ è½½å™¨) åŠ è½½æ•°æ® ---")

    try:
        # --- 1. åŠ è½½æ•°æ® ---
        X_train = pd.read_csv(cfg.data_train_eval_tabular, header=None)
        X_test = pd.read_csv(cfg.data_val_eval_tabular, header=None)
        
        # åŠ è½½æ ‡ç­¾
        y_train = torch.load(cfg.labels_train_eval_tabular).numpy()
        y_test = torch.load(cfg.labels_val_eval_tabular).numpy()

        print("    æ•°æ®åŠ è½½æˆåŠŸã€‚")

        # --- 2. åŠ è½½å­—æ®µé•¿åº¦ (ç”¨äºè‡ªåŠ¨è¯†åˆ«ç±»åˆ«ç‰¹å¾) ---
        # [!] å…³é”®ä¿®æ”¹ï¼šè¯»å– field_lengths
        all_field_lengths = torch.load(cfg.field_lengths_tabular)
        if isinstance(all_field_lengths, torch.Tensor):
            all_field_lengths = all_field_lengths.tolist()

        # ç®€å•çš„æ ¡éªŒ
        if X_train.shape[1] != len(all_field_lengths):
            print(f"ğŸ”´ é”™è¯¯ï¼šCSV åˆ—æ•° ({X_train.shape[1]}) ä¸ field_lengths é•¿åº¦ ({len(all_field_lengths)}) ä¸ä¸€è‡´ï¼")
            sys.exit(1)

        # --- 3. æ ‡ç­¾å¤„ç† (1-indexed -> 0-indexed) ---
        if task == 'classification':
            label_min = np.min(y_train)
            label_max = np.max(y_train)
            if label_min == 1 and label_max == cfg.num_classes:
                print(f"    [!] è­¦å‘Šï¼šæ£€æµ‹åˆ° 1-indexed æ ‡ç­¾ï¼Œæ­£åœ¨ä¿®æ­£...")
                y_train = y_train - 1
                y_test = y_test - 1

        # --- 4. è½¬æ¢åˆ†ç±»ç‰¹å¾ (æ ¸å¿ƒä¿®æ”¹) ---
        
        # è‡ªåŠ¨è¯†åˆ«ï¼šé•¿åº¦ > 1 çš„æ˜¯ç±»åˆ«ç‰¹å¾
        cat_indices = [i for i, length in enumerate(all_field_lengths) if length > 1]
        
        print(f"    è‡ªåŠ¨æ£€æµ‹åˆ° {len(cat_indices)} ä¸ªç±»åˆ«ç‰¹å¾ (æ ¹æ® field_lengths > 1)ã€‚")

        if len(cat_indices) > 0:
            # ä¸ºäº†é¿å… pandas çš„ SettingWithCopyWarning æˆ–ç±»å‹æ··æ·†ï¼Œ
            # å»ºè®®ç»™åˆ—é‡å‘½åä¸ºå­—ç¬¦ä¸²ï¼Œè¿™æ ·å¤„ç†èµ·æ¥æ›´æ¸…æ™°
            X_train.columns = [str(i) for i in range(X_train.shape[1])]
            X_test.columns  = [str(i) for i in range(X_test.shape[1])]

            # ä»…å°†æ£€æµ‹åˆ°çš„ç±»åˆ«åˆ—è½¬æ¢ä¸º 'category' ç±»å‹
            for idx in cat_indices:
                col_name = str(idx)
                # è½¬æ¢ä¸º category
                X_train[col_name] = X_train[col_name].astype('category')
                
                # å¯¹é½æµ‹è¯•é›† (å¤„ç†æœªçŸ¥ç±»åˆ«)
                # set_categories ç¡®ä¿æµ‹è¯•é›†å³ä½¿æœ‰æœªè§è¿‡çš„ç±»åˆ«ä¹Ÿä¸ä¼šæŠ¥é”™(ä¼šå˜æˆNaN)ï¼Œ
                # æˆ–è€…ç¡®ä¿å…¶ç±»åˆ«åˆ—è¡¨ä¸è®­ç»ƒé›†ä¸€è‡´
                X_test[col_name] = pd.Categorical(X_test[col_name], categories=X_train[col_name].cat.categories, ordered=False)
            
            print("    å·²å°†ç±»åˆ«ç‰¹å¾è½¬æ¢ä¸º pandas 'category' dtypeã€‚LightGBM å°†è‡ªåŠ¨è¯†åˆ«å®ƒä»¬ã€‚")
        else:
            print("    æœªæ£€æµ‹åˆ°ç±»åˆ«ç‰¹å¾ï¼Œæ‰€æœ‰åˆ—å°†ä½œä¸ºæ•°å€¼å¤„ç†ã€‚")

    except Exception as e:
        print(f"ğŸ”´ åŠ è½½æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_traceback()
        sys.exit(1)

    # --- 5. ç¡®å®šé—®é¢˜ç±»å‹ (ä¿æŒä¸å˜) ---
    print("-" * 30)
    if task == 'classification':
        num_classes = cfg.get('num_classes', len(np.unique(y_train))) 
        if num_classes == 2:
            problem_type = 'binary'; objective = 'binary'; num_class_param = {}; scoring_metric = 'roc_auc'
        else:
            problem_type = 'multiclass'; objective = 'multiclass'; num_class_param = {'num_class': num_classes}; scoring_metric = 'accuracy'
    elif task == 'regression':
        problem_type = 'regression'; objective = 'regression_l2'; num_class_param = {}; scoring_metric = 'neg_root_mean_squared_error'
    else:
        print(f"é”™è¯¯: ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹ '{task}'"); sys.exit(1)

    print(f"LGBM Objective: {objective}, Scoring: {scoring_metric}")
    
    return X_train, y_train, X_test, y_test, problem_type, objective, num_class_param, scoring_metric


def get_model_and_grid(problem_type, objective, num_class_param, seed):
    """
    æ ¹æ®é—®é¢˜ç±»å‹è·å–LGBMæ¨¡å‹å’Œå‚æ•°ç½‘æ ¼ã€‚
    """
    if problem_type in ['binary', 'multiclass']:
        model = lgb.LGBMClassifier(
            objective=objective,
            **num_class_param,
            random_state=seed,
            n_jobs=1,
            
            # --- â†“â†“â†“ å…³é”®ä¿®æ”¹ï¼šæ·»åŠ ä¸‹é¢ä¸€è¡Œ â†“â†“â†“ ---
            bagging_freq=1 # åªéœ€è¦åœ¨è¿™é‡Œæ¿€æ´» bagging
        )
    elif problem_type == 'regression':
        model = lgb.LGBMRegressor(
            objective=objective,
            random_state=seed,
            n_jobs=1,
            
            # --- â†“â†“â†“ å…³é”®ä¿®æ”¹ï¼šæ·»åŠ ä¸‹é¢ä¸€è¡Œ â†“â†“â†“ ---
            bagging_freq=1 # åªéœ€è¦åœ¨è¿™é‡Œæ¿€æ´» bagging
        )
    
    # --- â†“â†“â†“ å…³é”®ä¿®æ”¹ï¼šä¿®æ”¹ param_grid â†“â†“â†“ ---
    param_grid = {
        'num_leaves': [31, 127],
        'learning_rate': [0.01, 0.1],
        'min_child_samples': [20, 50, 100],
        'min_sum_hessian_in_leaf': [1e-3, 1e-2, 1e-1],
        
        # --- å°†é‡‡æ ·å‚æ•°æ·»åŠ åˆ°ç½‘æ ¼æœç´¢ä¸­ ---
        'feature_fraction': [0.8, 0.9], # æœç´¢ 80% æˆ– 90% çš„ç‰¹å¾
        'bagging_fraction': [0.8, 0.9]  # æœç´¢ 80% æˆ– 90% çš„æ•°æ®
    }
    
    return model, param_grid


def run_experiment(X_train, y_train, X_test, y_test, problem_type, objective, num_class_param, scoring_metric, seed):
    """
    ä½¿ç”¨ç»™å®šçš„éšæœºç§å­è¿è¡Œä¸€æ¬¡æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚
    """
    print(f"\n{'='*25} ---------------- éšæœºç§å­: {seed} ---------------- {'='*25}")
    
    model, param_grid = get_model_and_grid(problem_type, objective, num_class_param, seed)

    print(f"å¼€å§‹è¿›è¡Œç½‘æ ¼æœç´¢ (è¯„åˆ†æŒ‡æ ‡: {scoring_metric})...")
    
    if problem_type == 'regression':
        cv_splitter = KFold(n_splits=5, shuffle=True, random_state=seed)
    else:
        cv_splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)
    
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=scoring_metric,
        cv=cv_splitter,
        n_jobs=-1, # GridSearchCV ä½¿ç”¨æ‰€æœ‰æ ¸å¿ƒ
        verbose=1
    )
    grid_search.fit(X_train, y_train)

    print("ç½‘æ ¼æœç´¢å®Œæˆï¼")
    print(f"æ‰¾åˆ°çš„æœ€ä½³è¶…å‚æ•°: {grid_search.best_params_}")
    print(f"åœ¨äº¤å‰éªŒè¯ä¸­çš„æœ€ä½³ {scoring_metric}: {grid_search.best_score_:.4f}")
    print("-" * 30)

    print("ä½¿ç”¨æœ€ä½³æ¨¡å‹åœ¨æµ‹è¯•é›†(éªŒè¯é›†)ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    
    if problem_type in ['binary', 'multiclass']:
        y_pred_proba = best_model.predict_proba(X_test)
        acc = accuracy_score(y_test, y_pred)
        macro_f1 = f1_score(y_test, y_pred, average='macro')
        
        if problem_type == 'binary':
            auc = roc_auc_score(y_test, y_pred_proba[:, 1])
        else:
            auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')
        
        result_line = f"acc:{acc:.4f},auc:{auc:.4f},macro-F1:{macro_f1:.4f}"
    
    elif problem_type == 'regression':
        # --- ä¿®æ”¹ç‚¹ 2: å¢åŠ  MAE å’Œ R2 çš„è®¡ç®— ---
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        result_line = f"rmse:{rmse:.4f},mae:{mae:.4f},r2:{r2:.4f}"

    print("è¯„ä¼°ç»“æœ:")
    print(result_line)
    
    return result_line, grid_search.best_params_

@hydra.main(version_base=None, config_path="../configs", config_name="config")
def main(cfg: DictConfig):
    
    # [!] æ–°å¢æ¨¡å—ï¼šè§£ææ•°æ®è·¯å¾„ (ä¸ PyTorch è„šæœ¬ç›¸åŒ)
    # -----------------------------------------------------------------
    print("--- 1.A. æ­£åœ¨è§£ææ•°æ®è·¯å¾„ ---")
    data_root = cfg.get('data_base') 
    
    if data_root is not None:
        print(f"    æ£€æµ‹åˆ° 'data_root'ï¼Œå°†ä¸ºæ‰€æœ‰æ•°æ®æ–‡ä»¶æ·»åŠ å‰ç¼€: {data_root}")
        
        # å®šä¹‰åœ¨ .yaml ä¸­æ‰€æœ‰â€œéœ€è¦â€æ·»åŠ å‰ç¼€çš„è·¯å¾„é”®
        path_keys = [
            'labels_train', 'labels_val',
            'data_train_imaging', 'data_val_imaging',
            'data_train_tabular', 'data_val_tabular',
            'field_lengths_tabular',
            'data_train_eval_tabular', 'labels_train_eval_tabular',
            'data_val_eval_tabular', 'labels_val_eval_tabular',
            'data_test_eval_tabular', 'labels_test_eval_tabular',
            'data_train_eval_imaging', 'labels_train_eval_imaging',
            'data_val_eval_imaging', 'labels_val_eval_imaging',
            'data_test_eval_imaging', 'labels_test_eval_imaging'
        ]
        
        # éå†è¿™äº›é”®ï¼Œå¦‚æœå®ƒä»¬å­˜åœ¨äº cfg ä¸­ï¼Œåˆ™æ›´æ–°è·¯å¾„
        for key in path_keys:
            if key in cfg and cfg[key] is not None:
                original_path = cfg[key]
                absolute_path = os.path.join(data_root, original_path)
                cfg[key] = absolute_path # [!] ç›´æ¥ä¿®æ”¹ config å¯¹è±¡
            
    else:
        print("    æœªæä¾› 'data_root'ã€‚å°†å‡å®š config ä¸­çš„è·¯å¾„å·²ç»æ˜¯æ­£ç¡®çš„ (ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹äºCWD)ã€‚")
    # -----------------------------------------------------------------


    print("\n--- æœ€ç»ˆé…ç½® (è·¯å¾„å·²è§£æ): ---")
    print(OmegaConf.to_yaml(cfg))
    print("--------------------")
    print(f"Hydra å·¥ä½œç›®å½•: {os.getcwd()}")
    print("--------------------")

    # 1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ® (ç°åœ¨ cfg åŒ…å«ç»å¯¹è·¯å¾„)
    X_train, y_train, X_test, y_test, problem_type, objective, num_class_param, scoring_metric = load_and_preprocess_data(cfg)

    # å…è®¸ config.yaml æˆ–å‘½ä»¤è¡Œè¦†ç›– 'output_file_name'
    output_filename = 'result/lgb_results.txt'
    seeds = [2022, 2023, 2024]
    
    # 3. æ‰“å¼€æ–‡ä»¶å‡†å¤‡å†™å…¥ç»“æœ
    print(f"\nå‡†å¤‡å°†ç»“æœå†™å…¥åˆ°æ–‡ä»¶: {output_filename}")
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_filename), exist_ok=True)
    
    with open(output_filename, 'a') as f:
        f.write("--- æœ€ç»ˆé…ç½® ---\n")
        f.write(OmegaConf.to_yaml(cfg))
        f.write("-" * 30 + "\n\n")

        # 4. å¾ªç¯éå†æ‰€æœ‰éšæœºç§å­
        for seed in seeds:
            result_line, best_params = run_experiment(
                X_train, y_train, X_test, y_test,
                problem_type, objective, num_class_param, scoring_metric,
                seed
            )
            
            # 5. å†™å…¥ç»“æœ
            print(f"æ­£åœ¨å°†ç§å­ {seed} çš„ç»“æœå†™å…¥åˆ° {output_filename}...")
            f.write(f"seed:{seed}\n")
            f.write(f"best_params: {best_params}\n")
            f.write(result_line + "\n\n")

    print(f"\næ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ç»“æœå·²å…¨éƒ¨ä¿å­˜åœ¨ '{os.getcwd()}/{output_filename}' ä¸­ã€‚")


if __name__ == "__main__":
    main()