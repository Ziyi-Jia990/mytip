import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_squared_error
import sys
import torch
import os

# --- Hydra å¯¼å…¥ ---
import hydra
from omegaconf import DictConfig, OmegaConf

def load_and_preprocess_data(cfg: DictConfig):
    """
    (é‡æ„ç‰ˆ) 
    æ ¹æ® config å¯¹è±¡åŠ è½½å’Œé¢„å¤„ç†æ•°æ®ã€‚
    æ­¤å‡½æ•°å‡è®¾æ‰€æœ‰æ•°æ®é›†éƒ½å·²é€šè¿‡æ ‡å‡†é¢„å¤„ç†è„šæœ¬å¤„ç†ã€‚
    å®ƒå°†åŠ è½½å·²å¤„ç†çš„ .csv (ç‰¹å¾) å’Œ .pt (æ ‡ç­¾) æ–‡ä»¶ã€‚
    """
    task = cfg.get('task', 'classification')
    print(f"--- 1. æ­£åœ¨ä¸º Target: '{cfg.target}' (ä»»åŠ¡: {task}) (é€šç”¨LGBMåŠ è½½å™¨) åŠ è½½æ•°æ® ---")

    try:
        # --- 1. åŠ è½½æ•°æ® ---
        # ç›®æ ‡ï¼š
        # X_train, y_train = è®­ç»ƒé›† (ç”¨äº GridSearchCV)
        # X_test, y_test   = éªŒè¯é›† (ç”¨äºæœ€ç»ˆè¯„ä¼°)
        # æˆ‘ä»¬ä½¿ç”¨ä¸ PyTorch è„šæœ¬ç›¸åŒçš„ config é”®æ¥å®ç°è¿™ä¸€ç‚¹ã€‚

        print("    æ­£åœ¨åŠ è½½è®­ç»ƒé›† (X_train, y_train)...")
        # [!] ä½¿ç”¨ PyTorch è„šæœ¬çš„ config é”®
        X_train = pd.read_csv(cfg.data_train_eval_tabular, header=None)
        # [!] ç§»é™¤ 'weights_only' ä»¥å…¼å®¹æ—§ç‰ˆ PyTorch (å¦‚æœéœ€è¦)
        y_train_tensor = torch.load(cfg.labels_train_eval_tabular) 
        y_train = y_train_tensor.numpy()

        print("    æ­£åœ¨åŠ è½½éªŒè¯é›† (X_test, y_test)...")
        # [!] æˆ‘ä»¬ä½¿ç”¨ 'val' æ•°æ®é›†ä½œä¸º LGBM çš„ 'test' é›†ï¼Œä»¥ä¿æŒä¸ PyTorch å®éªŒçš„è¯„ä¼°ä¸€è‡´
        X_test = pd.read_csv(cfg.data_val_eval_tabular, header=None)
        y_test_tensor = torch.load(cfg.labels_val_eval_tabular)
        y_test = y_test_tensor.numpy()
        
        print("    æ•°æ®åŠ è½½æˆåŠŸã€‚")
        
        # --- 2. æ£€æŸ¥å¹¶ä¿®å¤ 1-indexed æ ‡ç­¾ ---
        if task == 'classification':
            label_min = np.min(y_train)
            label_max = np.max(y_train)
            
            # æ£€æŸ¥æ˜¯å¦ä¸º 1-indexed (ä¾‹å¦‚ 1, 2, 3, 4)
            if label_min == 1 and label_max == cfg.num_classes:
                print(f"    [!] è­¦å‘Šï¼šæ£€æµ‹åˆ° 1-indexed æ ‡ç­¾ (min={label_min}, max={label_max})ã€‚")
                print("        æ­£åœ¨å‡å» 1 ä½¿å…¶å˜ä¸º 0-indexedã€‚")
                y_train = y_train - 1 # è½¬æ¢ä¸º 0-indexed (0, 1, 2, 3)
                y_test = y_test - 1
            # æ£€æŸ¥æ˜¯å¦ä»ç„¶è¶Šç•Œ
            elif label_min < 0 or label_max >= cfg.num_classes:
                print(f"ğŸ”´ é”™è¯¯ï¼šæ ‡ç­¾è¶Šç•Œï¼")
                print(f"       æ¨¡å‹æœ‰ {cfg.num_classes} ä¸ªç±»åˆ« (é¢„æœŸ 0 åˆ° {cfg.num_classes - 1})")
                print(f"       ä½†æ ‡ç­¾ä¸­å‘ç° æœ€å°å€¼={label_min}, æœ€å¤§å€¼={label_max}")
                sys.exit(1)
        
        # --- 3. è½¬æ¢åˆ†ç±»ç‰¹å¾ ---
        # (ç§»é™¤åŸå§‹çš„ 'drop_cols' å’Œ 'align_cols'ï¼Œå› ä¸ºé¢„å¤„ç†å·²å®Œæˆ)
        
        num_con = cfg.num_con
        num_cat = cfg.num_cat
        total_features = num_con + num_cat
        
        if X_train.shape[1] != total_features:
            print(f"ğŸ”´ é”™è¯¯ï¼šåŠ è½½çš„ X_train æœ‰ {X_train.shape[1]} åˆ—, ä½† config é¢„æœŸ {total_features} (num_con+num_cat) åˆ—ã€‚")
            sys.exit(1)
        
        # è·å–åˆ†ç±»åˆ—çš„ *ç´¢å¼•* (ä¾‹å¦‚ [1, 2, 3, 4, 5])
        categorical_indices = list(range(num_con, total_features))
        
        if categorical_indices:
            print(f"    æ­£åœ¨å°† {len(categorical_indices)} ä¸ªç‰¹å¾è½¬æ¢ä¸º 'category' Dtype...")
            
            # æˆ‘ä»¬å¿…é¡»é‡å‘½ååˆ—ï¼Œå› ä¸ºåŸå§‹è„šæœ¬çš„ `pd.Categorical` ä¾èµ–äºåˆ—å
            col_names = [str(i) for i in range(total_features)]
            X_train.columns = col_names
            X_test.columns = col_names
            
            # è·å–åˆ†ç±»åˆ—çš„ *åç§°* (ä¾‹å¦‚ ['1', '2', '3', '4', '5'])
            categorical_cols = [str(i) for i in categorical_indices]

            for col in categorical_cols:
                X_train[col] = X_train[col].astype('category')
                # ä½¿ç”¨åŸå§‹è„šæœ¬ä¸­å¥å£®çš„æ–¹æ³•æ¥å¯¹é½æµ‹è¯•é›†
                X_test[col] = pd.Categorical(X_test[col], categories=X_train[col].cat.categories, ordered=False)
            
            print("    åˆ†ç±»ç‰¹å¾è½¬æ¢å®Œæˆã€‚")
        else:
            print("    æœªæ‰¾åˆ° (num_cat > 0) åˆ†ç±»ç‰¹å¾ã€‚")

    except FileNotFoundError as e:
        print(f"ğŸ”´ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {e.filename}ã€‚")
        print("    è¯·ç¡®ä¿ config ä¸­çš„è·¯å¾„æ­£ç¡®ï¼Œå¹¶ä¸”é¢„å¤„ç†è„šæœ¬å·²æˆåŠŸè¿è¡Œã€‚")
        sys.exit(1)
    except KeyError as e:
        print(f"ğŸ”´ é”™è¯¯ï¼šConfig æ–‡ä»¶ä¸­ç¼ºå°‘å…³é”®çš„é”®: {e}")
        print("    è¯·ç¡®ä¿ cfg åŒ…å« 'data_train_eval_tabular', 'labels_train_eval_tabular', 'data_val_eval_tabular', 'labels_val_eval_tabular', 'num_con', 'num_cat', 'task', 'num_classes'")
        sys.exit(1)
    except Exception as e:
        print(f"ğŸ”´ åŠ è½½æ•°æ®æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
        
    # --- 4. ç¡®å®šé—®é¢˜ç±»å‹å’Œè¯„ä¼°æŒ‡æ ‡ (æ­¤é€»è¾‘æ¥è‡ªåŸå§‹è„šæœ¬ï¼Œæ˜¯æ­£ç¡®çš„) ---
    print("-" * 30)
    if task == 'classification':
        # é‡æ–°è·å–ï¼Œä»¥é˜²ä¸‡ä¸€
        num_classes = cfg.get('num_classes', len(np.unique(y_train))) 
        
        if num_classes == 2:
            print(f"æ£€æµ‹åˆ°äºŒåˆ†ç±»é—®é¢˜ (num_classes={num_classes})ã€‚")
            problem_type = 'binary'
            objective = 'binary'
            num_class_param = {}
            scoring_metric = 'roc_auc'
        else:
            print(f"æ£€æµ‹åˆ°å¤šåˆ†ç±»é—®é¢˜ (num_classes={num_classes})ã€‚")
            problem_type = 'multiclass'
            objective = 'multiclass'
            num_class_param = {'num_class': num_classes}
            scoring_metric = 'accuracy'
    
    elif task == 'regression':
        print("æ£€æµ‹åˆ°å›å½’é—®é¢˜ã€‚")
        problem_type = 'regression'
        objective = 'regression_l2'
        num_class_param = {}
        scoring_metric = 'neg_root_mean_squared_error'
    
    else:
        print(f"é”™è¯¯: ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹ '{task}'ã€‚")
        sys.exit(1)

    print(f"LGBM Objective: {objective}, GridSearchCV Scoring: {scoring_metric}")
    print("-" * 30)

    return X_train, y_train, X_test, y_test, problem_type, objective, num_class_param, scoring_metric


def get_model_and_grid(problem_type, objective, num_class_param, seed):
    """
    æ ¹æ®é—®é¢˜ç±»å‹è·å–LGBMæ¨¡å‹å’Œå‚æ•°ç½‘æ ¼ã€‚
    """
    if problem_type in ['binary', 'multiclass']:
        model = lgb.LGBMClassifier(
            objective=objective,
            **num_class_param,
            random_state=seed,
            n_jobs=1,
            
            # --- â†“â†“â†“ å…³é”®ä¿®æ”¹ï¼šæ·»åŠ ä¸‹é¢ä¸€è¡Œ â†“â†“â†“ ---
            bagging_freq=1 # åªéœ€è¦åœ¨è¿™é‡Œæ¿€æ´» bagging
        )
    elif problem_type == 'regression':
        model = lgb.LGBMRegressor(
            objective=objective,
            random_state=seed,
            n_jobs=1,
            
            # --- â†“â†“â†“ å…³é”®ä¿®æ”¹ï¼šæ·»åŠ ä¸‹é¢ä¸€è¡Œ â†“â†“â†“ ---
            bagging_freq=1 # åªéœ€è¦åœ¨è¿™é‡Œæ¿€æ´» bagging
        )
    
    # --- â†“â†“â†“ å…³é”®ä¿®æ”¹ï¼šä¿®æ”¹ param_grid â†“â†“â†“ ---
    param_grid = {
        'num_leaves': [31, 127],
        'learning_rate': [0.01, 0.1],
        'min_child_samples': [20, 50, 100],
        'min_sum_hessian_in_leaf': [1e-3, 1e-2, 1e-1],
        
        # --- å°†é‡‡æ ·å‚æ•°æ·»åŠ åˆ°ç½‘æ ¼æœç´¢ä¸­ ---
        'feature_fraction': [0.8, 0.9], # æœç´¢ 80% æˆ– 90% çš„ç‰¹å¾
        'bagging_fraction': [0.8, 0.9]  # æœç´¢ 80% æˆ– 90% çš„æ•°æ®
    }
    
    return model, param_grid


def run_experiment(X_train, y_train, X_test, y_test, problem_type, objective, num_class_param, scoring_metric, seed):
    """
    ä½¿ç”¨ç»™å®šçš„éšæœºç§å­è¿è¡Œä¸€æ¬¡æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚
    """
    print(f"\n{'='*25} ---------------- éšæœºç§å­: {seed} ---------------- {'='*25}")
    
    model, param_grid = get_model_and_grid(problem_type, objective, num_class_param, seed)

    print(f"å¼€å§‹è¿›è¡Œç½‘æ ¼æœç´¢ (è¯„åˆ†æŒ‡æ ‡: {scoring_metric})...")
    
    if problem_type == 'regression':
        cv_splitter = KFold(n_splits=5, shuffle=True, random_state=seed)
    else:
        cv_splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)
    
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=scoring_metric,
        cv=cv_splitter,
        n_jobs=-1, # GridSearchCV ä½¿ç”¨æ‰€æœ‰æ ¸å¿ƒ
        verbose=1
    )
    grid_search.fit(X_train, y_train)

    print("ç½‘æ ¼æœç´¢å®Œæˆï¼")
    print(f"æ‰¾åˆ°çš„æœ€ä½³è¶…å‚æ•°: {grid_search.best_params_}")
    print(f"åœ¨äº¤å‰éªŒè¯ä¸­çš„æœ€ä½³ {scoring_metric}: {grid_search.best_score_:.4f}")
    print("-" * 30)

    print("ä½¿ç”¨æœ€ä½³æ¨¡å‹åœ¨æµ‹è¯•é›†(éªŒè¯é›†)ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    
    if problem_type in ['binary', 'multiclass']:
        y_pred_proba = best_model.predict_proba(X_test)
        acc = accuracy_score(y_test, y_pred)
        macro_f1 = f1_score(y_test, y_pred, average='macro')
        
        if problem_type == 'binary':
            auc = roc_auc_score(y_test, y_pred_proba[:, 1])
        else:
            auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')
        
        result_line = f"acc:{acc:.4f},auc:{auc:.4f},macro-F1:{macro_f1:.4f}"
    
    elif problem_type == 'regression':
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        result_line = f"rmse:{rmse:.4f}"

    print("è¯„ä¼°ç»“æœ:")
    print(result_line)
    
    return result_line, grid_search.best_params_

@hydra.main(version_base=None, config_path="../configs", config_name="config")
def main(cfg: DictConfig):
    
    # [!] æ–°å¢æ¨¡å—ï¼šè§£ææ•°æ®è·¯å¾„ (ä¸ PyTorch è„šæœ¬ç›¸åŒ)
    # -----------------------------------------------------------------
    print("--- 1.A. æ­£åœ¨è§£ææ•°æ®è·¯å¾„ ---")
    data_root = cfg.get('data_base') 
    
    if data_root is not None:
        print(f"    æ£€æµ‹åˆ° 'data_root'ï¼Œå°†ä¸ºæ‰€æœ‰æ•°æ®æ–‡ä»¶æ·»åŠ å‰ç¼€: {data_root}")
        
        # å®šä¹‰åœ¨ .yaml ä¸­æ‰€æœ‰â€œéœ€è¦â€æ·»åŠ å‰ç¼€çš„è·¯å¾„é”®
        path_keys = [
            'labels_train', 'labels_val',
            'data_train_imaging', 'data_val_imaging',
            'data_train_tabular', 'data_val_tabular',
            'field_lengths_tabular',
            'data_train_eval_tabular', 'labels_train_eval_tabular',
            'data_val_eval_tabular', 'labels_val_eval_tabular',
            'data_test_eval_tabular', 'labels_test_eval_tabular',
            'data_train_eval_imaging', 'labels_train_eval_imaging',
            'data_val_eval_imaging', 'labels_val_eval_imaging',
            'data_test_eval_imaging', 'labels_test_eval_imaging'
        ]
        
        # éå†è¿™äº›é”®ï¼Œå¦‚æœå®ƒä»¬å­˜åœ¨äº cfg ä¸­ï¼Œåˆ™æ›´æ–°è·¯å¾„
        for key in path_keys:
            if key in cfg and cfg[key] is not None:
                original_path = cfg[key]
                absolute_path = os.path.join(data_root, original_path)
                cfg[key] = absolute_path # [!] ç›´æ¥ä¿®æ”¹ config å¯¹è±¡
            
    else:
        print("    æœªæä¾› 'data_root'ã€‚å°†å‡å®š config ä¸­çš„è·¯å¾„å·²ç»æ˜¯æ­£ç¡®çš„ (ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹äºCWD)ã€‚")
    # -----------------------------------------------------------------


    print("\n--- æœ€ç»ˆé…ç½® (è·¯å¾„å·²è§£æ): ---")
    print(OmegaConf.to_yaml(cfg))
    print("--------------------")
    print(f"Hydra å·¥ä½œç›®å½•: {os.getcwd()}")
    print("--------------------")

    # 1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ® (ç°åœ¨ cfg åŒ…å«ç»å¯¹è·¯å¾„)
    X_train, y_train, X_test, y_test, problem_type, objective, num_class_param, scoring_metric = load_and_preprocess_data(cfg)

    # å…è®¸ config.yaml æˆ–å‘½ä»¤è¡Œè¦†ç›– 'output_file_name'
    output_filename = '/home/debian/TIP/mymodel/result/lgb_results.txt'
    seeds = [2022, 2023, 2024]
    
    # 3. æ‰“å¼€æ–‡ä»¶å‡†å¤‡å†™å…¥ç»“æœ
    print(f"\nå‡†å¤‡å°†ç»“æœå†™å…¥åˆ°æ–‡ä»¶: {output_filename}")
    with open(output_filename, 'a') as f:
        f.write("--- æœ€ç»ˆé…ç½® ---\n")
        f.write(OmegaConf.to_yaml(cfg))
        f.write("-" * 30 + "\n\n")

        # 4. å¾ªç¯éå†æ‰€æœ‰éšæœºç§å­
        for seed in seeds:
            result_line, best_params = run_experiment(
                X_train, y_train, X_test, y_test,
                problem_type, objective, num_class_param, scoring_metric,
                seed
            )
            
            # 5. å†™å…¥ç»“æœ
            print(f"æ­£åœ¨å°†ç§å­ {seed} çš„ç»“æœå†™å…¥åˆ° {output_filename}...")
            f.write(f"seed:{seed}\n")
            f.write(f"best_params: {best_params}\n")
            f.write(result_line + "\n\n")

    print(f"\næ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ç»“æœå·²å…¨éƒ¨ä¿å­˜åœ¨ '{os.getcwd()}/{output_filename}' ä¸­ã€‚")


if __name__ == "__main__":
    main()